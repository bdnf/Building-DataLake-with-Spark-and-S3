{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Chicago Taxi and Weather Conditions Data Modeling\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    " \n",
    "\n",
    "#### Project Summary\n",
    "In the following project we will try to explore the taxi demand in Chicago based on Temperature data.\n",
    "\n",
    "We would like to explore the following questions:\n",
    "\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "In the current project, a public Chicago Taxi Trips dataset will be used.\n",
    "We will take information for last several months and combine it with weather data, and weather conditions during each day, recorded in Chicago OHARE INTERNATIONAL AIRPORT.\n",
    "Data will be processed and ready to be loaded for further analysis.\n",
    "\n",
    "Chicago taxi dataset is a public dataset that can be found at: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\n",
    "\n",
    "The weather dataset can be found here: https://www.ncdc.noaa.gov/cdo-web/datasets\n",
    "\n",
    "ETL process will be performed on the AWS EMR Spark cluster, and data will be loaded to Data Lake only during last step. All processing steps will happen in memory.\n",
    "\n",
    "Data will then be saved in S3 Data Lake in Parquet format, that would allow it to be easily queryable with tools like Presto, AWS Athena or AWS Glue.\n",
    "\n",
    "Last step can be changed if needed, for example for data to be written directly to Redshift cluster.\n",
    "<br>However, all the tables should be created beforehand performing such option.\n",
    "\n",
    "The final tables will have a format that can support running predefined queries.\n",
    "<br>This means, it could be beneficial to load the data into NoSQL database, such as Cassandra, that allows exactly that kind of queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicago taxi Dataset will be described in **section 4**, however we provide general outline of the whole dataset below.\n",
    "\n",
    "![dataset-info](./assets/dataset.png)\n",
    "\n",
    "![columns](./assets/dataset-columns.png)\n",
    "\n",
    "# Chicago Community Areas\n",
    "\n",
    "Chicago is officially divided into 77 community areas (zones). More can be learned via this link: https://en.wikipedia.org/wiki/Community_areas_in_Chicago\n",
    "\n",
    "Data can be populated with real area names, however for many purposes, keeping number mapping makes more sense.\n",
    "\n",
    "![columns](./assets/chicago-areas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local cluster setup prepequisites\n",
    "Some preinstallation may be required if Notebook is run from local cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8MB 45.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 42.1MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=0b296db2c8d0ce2baf7660a963ae55a7ea99fa7747303f68d13e01d4165216f4\n",
      "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 32.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package multiarch-support.\n",
      "(Reading database ... 18426 files and directories currently installed.)\n",
      "Preparing to unpack .../multiarch-support_2.27-3ubuntu1_amd64.deb ...\n",
      "Unpacking multiarch-support (2.27-3ubuntu1) ...\n",
      "Setting up multiarch-support (2.27-3ubuntu1) ...\n",
      "Selecting previously unselected package libxau6:amd64.\n",
      "(Reading database ... 18429 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libxau6_1%3a1.0.8-1_amd64.deb ...\n",
      "Unpacking libxau6:amd64 (1:1.0.8-1) ...\n",
      "Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../01-libbsd0_0.8.7-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../02-libxdmcp6_1%3a1.1.2-3_amd64.deb ...\n",
      "Unpacking libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../03-libxcb1_1.13-2~ubuntu18.04_amd64.deb ...\n",
      "Unpacking libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../04-libx11-data_2%3a1.6.4-3ubuntu0.2_all.deb ...\n",
      "Unpacking libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../05-libx11-6_2%3a1.6.4-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libxext6:amd64.\n",
      "Preparing to unpack .../06-libxext6_2%3a1.3.3-1_amd64.deb ...\n",
      "Unpacking libxext6:amd64 (2:1.3.3-1) ...\n",
      "Selecting previously unselected package libjpeg-turbo8:amd64.\n",
      "Preparing to unpack .../07-libjpeg-turbo8_1.5.2-0ubuntu5.18.04.3_amd64.deb ...\n",
      "Unpacking libjpeg-turbo8:amd64 (1.5.2-0ubuntu5.18.04.3) ...\n",
      "Selecting previously unselected package ucf.\n",
      "Preparing to unpack .../08-ucf_3.0038_all.deb ...\n",
      "Moving old data out of the way\n",
      "Unpacking ucf (3.0038) ...\n",
      "Selecting previously unselected package libpng16-16:amd64.\n",
      "Preparing to unpack .../09-libpng16-16_1.6.34-1ubuntu0.18.04.2_amd64.deb ...\n",
      "Unpacking libpng16-16:amd64 (1.6.34-1ubuntu0.18.04.2) ...\n",
      "Selecting previously unselected package java-common.\n",
      "Preparing to unpack .../10-java-common_0.68ubuntu1~18.04.1_all.deb ...\n",
      "Unpacking java-common (0.68ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package libavahi-common-data:amd64.\n",
      "Preparing to unpack .../11-libavahi-common-data_0.7-3.1ubuntu1.2_amd64.deb ...\n",
      "Unpacking libavahi-common-data:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Selecting previously unselected package libavahi-common3:amd64.\n",
      "Preparing to unpack .../12-libavahi-common3_0.7-3.1ubuntu1.2_amd64.deb ...\n",
      "Unpacking libavahi-common3:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Selecting previously unselected package libavahi-client3:amd64.\n",
      "Preparing to unpack .../13-libavahi-client3_0.7-3.1ubuntu1.2_amd64.deb ...\n",
      "Unpacking libavahi-client3:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Selecting previously unselected package libcups2:amd64.\n",
      "Preparing to unpack .../14-libcups2_2.2.7-1ubuntu2.7_amd64.deb ...\n",
      "Unpacking libcups2:amd64 (2.2.7-1ubuntu2.7) ...\n",
      "Selecting previously unselected package liblcms2-2:amd64.\n",
      "Preparing to unpack .../15-liblcms2-2_2.9-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking liblcms2-2:amd64 (2.9-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libjpeg8:amd64.\n",
      "Preparing to unpack .../16-libjpeg8_8c-2ubuntu8_amd64.deb ...\n",
      "Unpacking libjpeg8:amd64 (8c-2ubuntu8) ...\n",
      "Selecting previously unselected package libfreetype6:amd64.\n",
      "Preparing to unpack .../17-libfreetype6_2.8.1-2ubuntu2_amd64.deb ...\n",
      "Unpacking libfreetype6:amd64 (2.8.1-2ubuntu2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../18-fonts-dejavu-core_2.37-1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-1) ...\n",
      "Selecting previously unselected package fontconfig-config.\n",
      "Preparing to unpack .../19-fontconfig-config_2.12.6-0ubuntu2_all.deb ...\n",
      "Unpacking fontconfig-config (2.12.6-0ubuntu2) ...\n",
      "Selecting previously unselected package libfontconfig1:amd64.\n",
      "Preparing to unpack .../20-libfontconfig1_2.12.6-0ubuntu2_amd64.deb ...\n",
      "Unpacking libfontconfig1:amd64 (2.12.6-0ubuntu2) ...\n",
      "Selecting previously unselected package libnspr4:amd64.\n",
      "Preparing to unpack .../21-libnspr4_2%3a4.18-1ubuntu1_amd64.deb ...\n",
      "Unpacking libnspr4:amd64 (2:4.18-1ubuntu1) ...\n",
      "Selecting previously unselected package libnss3:amd64.\n",
      "Preparing to unpack .../22-libnss3_2%3a3.35-2ubuntu2.7_amd64.deb ...\n",
      "Unpacking libnss3:amd64 (2:3.35-2ubuntu2.7) ...\n",
      "Selecting previously unselected package libpcsclite1:amd64.\n",
      "Preparing to unpack .../23-libpcsclite1_1.8.23-1_amd64.deb ...\n",
      "Unpacking libpcsclite1:amd64 (1.8.23-1) ...\n",
      "Selecting previously unselected package libxi6:amd64.\n",
      "Preparing to unpack .../24-libxi6_2%3a1.7.9-1_amd64.deb ...\n",
      "Unpacking libxi6:amd64 (2:1.7.9-1) ...\n",
      "Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../25-libxrender1_1%3a0.9.10-1_amd64.deb ...\n",
      "Unpacking libxrender1:amd64 (1:0.9.10-1) ...\n",
      "Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../26-x11-common_1%3a7.7+19ubuntu7.1_all.deb ...\n",
      "dpkg-query: no packages found matching nux-tools\n",
      "Unpacking x11-common (1:7.7+19ubuntu7.1) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../27-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
      "Preparing to unpack .../28-openjdk-8-jre-headless_8u242-b08-0ubuntu3~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jre-headless:amd64 (8u242-b08-0ubuntu3~18.04) ...\n",
      "Selecting previously unselected package ca-certificates-java.\n",
      "Preparing to unpack .../29-ca-certificates-java_20180516ubuntu1~18.04.1_all.deb ...\n",
      "Unpacking ca-certificates-java (20180516ubuntu1~18.04.1) ...\n",
      "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
      "Preparing to unpack .../30-openjdk-8-jdk-headless_8u242-b08-0ubuntu3~18.04_amd64.deb ...\n",
      "Unpacking openjdk-8-jdk-headless:amd64 (8u242-b08-0ubuntu3~18.04) ...\n",
      "Setting up libpng16-16:amd64 (1.6.34-1ubuntu0.18.04.2) ...\n",
      "Setting up liblcms2-2:amd64 (2.9-1ubuntu0.1) ...\n",
      "Setting up libpcsclite1:amd64 (1.8.23-1) ...\n",
      "Setting up fonts-dejavu-core (2.37-1) ...\n",
      "Setting up java-common (0.68ubuntu1~18.04.1) ...\n",
      "Setting up libjpeg-turbo8:amd64 (1.5.2-0ubuntu5.18.04.3) ...\n",
      "Setting up libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "Setting up libnspr4:amd64 (2:4.18-1ubuntu1) ...\n",
      "Setting up ucf (3.0038) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Setting up libfreetype6:amd64 (2.8.1-2ubuntu2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Setting up libxdmcp6:amd64 (1:1.1.2-3) ...\n",
      "Setting up x11-common (1:7.7+19ubuntu7.1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
      "debconf: falling back to frontend: Readline\n",
      "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Processing triggers for ca-certificates (20180409) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "done.\n",
      "Setting up libx11-data (2:1.6.4-3ubuntu0.2) ...\n",
      "Setting up libxau6:amd64 (1:1.0.8-1) ...\n",
      "Setting up libavahi-common-data:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Setting up libjpeg8:amd64 (8c-2ubuntu8) ...\n",
      "Setting up fontconfig-config (2.12.6-0ubuntu2) ...\n",
      "Setting up libnss3:amd64 (2:3.35-2ubuntu2.7) ...\n",
      "Setting up libavahi-common3:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Setting up libxcb1:amd64 (1.13-2~ubuntu18.04) ...\n",
      "Setting up libfontconfig1:amd64 (2.12.6-0ubuntu2) ...\n",
      "Setting up libx11-6:amd64 (2:1.6.4-3ubuntu0.2) ...\n",
      "Setting up libxrender1:amd64 (1:0.9.10-1) ...\n",
      "Setting up libavahi-client3:amd64 (0.7-3.1ubuntu1.2) ...\n",
      "Setting up libcups2:amd64 (2.2.7-1ubuntu2.7) ...\n",
      "Setting up libxext6:amd64 (2:1.3.3-1) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
      "Setting up libxi6:amd64 (2:1.7.9-1) ...\n",
      "Setting up openjdk-8-jre-headless:amd64 (8u242-b08-0ubuntu3~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode\n",
      "Setting up ca-certificates-java (20180516ubuntu1~18.04.1) ...\n",
      "head: cannot open '/etc/ssl/certs/java/cacerts' for reading: No such file or directory\n",
      "Adding debian:ACCVRAIZ1.pem\n",
      "Adding debian:AC_RAIZ_FNMT-RCM.pem\n",
      "Adding debian:Actalis_Authentication_Root_CA.pem\n",
      "Adding debian:AddTrust_External_Root.pem\n",
      "Adding debian:AffirmTrust_Commercial.pem\n",
      "Adding debian:AffirmTrust_Networking.pem\n",
      "Adding debian:AffirmTrust_Premium.pem\n",
      "Adding debian:AffirmTrust_Premium_ECC.pem\n",
      "Adding debian:Amazon_Root_CA_1.pem\n",
      "Adding debian:Amazon_Root_CA_2.pem\n",
      "Adding debian:Amazon_Root_CA_3.pem\n",
      "Adding debian:Amazon_Root_CA_4.pem\n",
      "Adding debian:Atos_TrustedRoot_2011.pem\n",
      "Adding debian:Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem\n",
      "Adding debian:Baltimore_CyberTrust_Root.pem\n",
      "Adding debian:Buypass_Class_2_Root_CA.pem\n",
      "Adding debian:Buypass_Class_3_Root_CA.pem\n",
      "Adding debian:CA_Disig_Root_R2.pem\n",
      "Adding debian:CFCA_EV_ROOT.pem\n",
      "Adding debian:COMODO_Certification_Authority.pem\n",
      "Adding debian:COMODO_ECC_Certification_Authority.pem\n",
      "Adding debian:COMODO_RSA_Certification_Authority.pem\n",
      "Adding debian:Certigna.pem\n",
      "Adding debian:Certinomis_-_Root_CA.pem\n",
      "Adding debian:Certplus_Class_2_Primary_CA.pem\n",
      "Adding debian:Certplus_Root_CA_G1.pem\n",
      "Adding debian:Certplus_Root_CA_G2.pem\n",
      "Adding debian:Certum_Trusted_Network_CA.pem\n",
      "Adding debian:Certum_Trusted_Network_CA_2.pem\n",
      "Adding debian:Chambers_of_Commerce_Root_-_2008.pem\n",
      "Adding debian:Comodo_AAA_Services_root.pem\n",
      "Adding debian:Cybertrust_Global_Root.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_2009.pem\n",
      "Adding debian:D-TRUST_Root_Class_3_CA_2_EV_2009.pem\n",
      "Adding debian:DST_Root_CA_X3.pem\n",
      "Adding debian:Deutsche_Telekom_Root_CA_2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_CA.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G2.pem\n",
      "Adding debian:DigiCert_Assured_ID_Root_G3.pem\n",
      "Adding debian:DigiCert_Global_Root_CA.pem\n",
      "Adding debian:DigiCert_Global_Root_G2.pem\n",
      "Adding debian:DigiCert_Global_Root_G3.pem\n",
      "Adding debian:DigiCert_High_Assurance_EV_Root_CA.pem\n",
      "Adding debian:DigiCert_Trusted_Root_G4.pem\n",
      "Adding debian:E-Tugra_Certification_Authority.pem\n",
      "Adding debian:EC-ACC.pem\n",
      "Adding debian:GDCA_TrustAUTH_R5_ROOT.pem\n",
      "Adding debian:EE_Certification_Centre_Root_CA.pem\n",
      "Adding debian:Entrust.net_Premium_2048_Secure_Server_CA.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_EC1.pem\n",
      "Adding debian:Entrust_Root_Certification_Authority_-_G2.pem\n",
      "Adding debian:GeoTrust_Global_CA.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority_-_G2.pem\n",
      "Adding debian:GeoTrust_Primary_Certification_Authority_-_G3.pem\n",
      "Adding debian:GeoTrust_Universal_CA.pem\n",
      "Adding debian:GeoTrust_Universal_CA_2.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R4.pem\n",
      "Adding debian:GlobalSign_ECC_Root_CA_-_R5.pem\n",
      "Adding debian:GlobalSign_Root_CA.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R2.pem\n",
      "Adding debian:GlobalSign_Root_CA_-_R3.pem\n",
      "Adding debian:Global_Chambersign_Root_-_2008.pem\n",
      "Adding debian:Go_Daddy_Class_2_CA.pem\n",
      "Adding debian:Go_Daddy_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2011.pem\n",
      "Adding debian:Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem\n",
      "Adding debian:Hongkong_Post_Root_CA_1.pem\n",
      "Adding debian:ISRG_Root_X1.pem\n",
      "Adding debian:IdenTrust_Commercial_Root_CA_1.pem\n",
      "Adding debian:IdenTrust_Public_Sector_Root_CA_1.pem\n",
      "Adding debian:Izenpe.com.pem\n",
      "Adding debian:LuxTrust_Global_Root_2.pem\n",
      "Adding debian:Microsec_e-Szigno_Root_CA_2009.pem\n",
      "Adding debian:NetLock_Arany_=Class_Gold=_Főtanúsítvány.pem\n",
      "Adding debian:Network_Solutions_Certificate_Authority.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GA_CA.pem\n",
      "Adding debian:OISTE_WISeKey_Global_Root_GB_CA.pem\n",
      "Adding debian:OpenTrust_Root_CA_G1.pem\n",
      "Adding debian:OpenTrust_Root_CA_G2.pem\n",
      "Adding debian:OpenTrust_Root_CA_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA.pem\n",
      "Adding debian:QuoVadis_Root_CA_1_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_2.pem\n",
      "Adding debian:QuoVadis_Root_CA_2_G3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3.pem\n",
      "Adding debian:QuoVadis_Root_CA_3_G3.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_EV_Root_Certification_Authority_RSA_R2.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_ECC.pem\n",
      "Adding debian:SSL.com_Root_Certification_Authority_RSA.pem\n",
      "Adding debian:SZAFIR_ROOT_CA2.pem\n",
      "Adding debian:SecureSign_RootCA11.pem\n",
      "Adding debian:SecureTrust_CA.pem\n",
      "Adding debian:Secure_Global_CA.pem\n",
      "Adding debian:Security_Communication_RootCA2.pem\n",
      "Adding debian:Security_Communication_Root_CA.pem\n",
      "Adding debian:Sonera_Class_2_Root_CA.pem\n",
      "Adding debian:Staat_der_Nederlanden_EV_Root_CA.pem\n",
      "Adding debian:Staat_der_Nederlanden_Root_CA_-_G2.pem\n",
      "Adding debian:Staat_der_Nederlanden_Root_CA_-_G3.pem\n",
      "Adding debian:Starfield_Class_2_CA.pem\n",
      "Adding debian:Starfield_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:Starfield_Services_Root_Certificate_Authority_-_G2.pem\n",
      "Adding debian:SwissSign_Gold_CA_-_G2.pem\n",
      "Adding debian:SwissSign_Silver_CA_-_G2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_2.pem\n",
      "Adding debian:T-TeleSec_GlobalRoot_Class_3.pem\n",
      "Adding debian:TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem\n",
      "Adding debian:TWCA_Global_Root_CA.pem\n",
      "Adding debian:TWCA_Root_Certification_Authority.pem\n",
      "Adding debian:Taiwan_GRCA.pem\n",
      "Adding debian:TeliaSonera_Root_CA_v1.pem\n",
      "Adding debian:TrustCor_ECA-1.pem\n",
      "Adding debian:TrustCor_RootCert_CA-1.pem\n",
      "Adding debian:TrustCor_RootCert_CA-2.pem\n",
      "Adding debian:Trustis_FPS_Root_CA.pem\n",
      "Adding debian:TÜRKTRUST_Elektronik_Sertifika_Hizmet_Sağlayıcısı_H5.pem\n",
      "Adding debian:USERTrust_ECC_Certification_Authority.pem\n",
      "Adding debian:USERTrust_RSA_Certification_Authority.pem\n",
      "Adding debian:VeriSign_Class_3_Public_Primary_Certification_Authority_-_G4.pem\n",
      "Adding debian:VeriSign_Class_3_Public_Primary_Certification_Authority_-_G5.pem\n",
      "Adding debian:VeriSign_Universal_Root_Certification_Authority.pem\n",
      "Adding debian:Verisign_Class_3_Public_Primary_Certification_Authority_-_G3.pem\n",
      "Adding debian:Visa_eCommerce_Root.pem\n",
      "Adding debian:XRamp_Global_CA_Root.pem\n",
      "Adding debian:certSIGN_ROOT_CA.pem\n",
      "Adding debian:ePKI_Root_Certification_Authority.pem\n",
      "Adding debian:thawte_Primary_Root_CA.pem\n",
      "Adding debian:thawte_Primary_Root_CA_-_G2.pem\n",
      "Adding debian:thawte_Primary_Root_CA_-_G3.pem\n",
      "done.\n",
      "Setting up openjdk-8-jdk-headless:amd64 (8u242-b08-0ubuntu3~18.04) ...\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javap to provide /usr/bin/javap (javap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jar to provide /usr/bin/jar (jar) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode\n",
      "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Processing triggers for ca-certificates (20180409) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "\n",
      "done.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "wget is already the newest version (1.19.4-1ubuntu2.2).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install wget -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data can be downloaded via official API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-21 10:32:09--  https://data.cityofchicago.org/resource/wrvz-psew.csv?%24where=trip_start_timestamp%20%3E=%20'2018-01-01T00:00:00'&%24limit=100000000\n",
      "Resolving data.cityofchicago.org (data.cityofchicago.org)... 52.206.140.205, 52.206.68.26, 52.206.140.199\n",
      "Connecting to data.cityofchicago.org (data.cityofchicago.org)|52.206.140.205|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘/storage/taxi_trips_100m_rows.csv’\n",
      "\n",
      "/storage/taxi_trips     [ <=>                ]  16.38G  11.2MB/s    in 24m 49s \n",
      "\n",
      "2020-02-21 10:58:08 (11.3 MB/s) - ‘/storage/taxi_trips_100m_rows.csv’ saved [17584118950]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O /storage/taxi_trips_100m_rows.csv \"https://data.cityofchicago.org/resource/wrvz-psew.csv?%24where=trip_start_timestamp%20>=%20'2018-01-01T00:00:00'&%24limit=100000000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1a. Loading Taxi Data\n",
    "\n",
    "Chicago Taxi Dataset is very large (around 170 Gb on the current web site), and we will use part of this data with 100 million rows, that currently has size of around 11 Gb.\n",
    "\n",
    "The data will contain trips only from 2018 till present day Febrauary 2020 (currently available data upper date is 5 February), and for exploratory purposes will be limited to 40 million rows.\n",
    "\n",
    "Weather dataset contains weather recording from 1953 till today. Here, we only use a subset of this data from 2018 till February 2020.\n",
    "\n",
    "Considering large amount of data, the use of S3 buckets for storage is generally is very good idea, especially if ETL pipelines are performed in EMR cluster, as this allow faster data retrieval and load withing AWS internal network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec8554c98a74643823d1b20e723f5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1582290877179_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-46-120.ec2.internal:20888/proxy/application_1582290877179_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-46-120.ec2.internal:8042/node/containerlogs/container_1582290877179_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Buckets. Change the name of the bucket where data is stored\n",
    "\n",
    "#trips_data = \"./storage/taxi_trips_100m_rows.csv\"\n",
    "trips_data = 's3://taxi-big-data-for-capstone/taxi_trips_100m_rows.csv'\n",
    "weather_data = \"s3a://taxi-big-data-sample/chicago_weather_extended_data.csv\" #\"./storage/chicago_weather_extended_data.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to perform analysis and ETL the dataset should be loaded first.\n",
    "\n",
    "Next we check if the data is correct, and if some columns have missing data.\n",
    "\n",
    "Additionally, first 5 rows of each datset, schema and column list are shown.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9415a10a0ea54979a8dd7e55a0f123a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run outside of EMR cluster, create Spark Session by hand\n",
    "AWS_ACCESS_KEY = os.getenv('AWS_ACCESS_KEY_ID', None)\n",
    "AWS_SECRET_KEY = os.getenv('AWS_SECRET_KEY', None)\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"returns: Spark Session object\n",
    "    \n",
    "    Description: Factory function that creates Spark Session objects.\n",
    "    \n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .config(\"fs.s3a.awsAccessKeyId\", AWS_ACCESS_KEY)\\\n",
    "        .config(\"fs.s3a.awsSecretAccessKey\", AWS_SECRET_KEY)\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e73bf1006542d89de7c144a7544604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxi_trips = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").csv(trips_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read the dataset and explore how does the data actually look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_centroid_latitude</th>\n",
       "      <th>pickup_centroid_longitude</th>\n",
       "      <th>pickup_centroid_location</th>\n",
       "      <th>dropoff_centroid_latitude</th>\n",
       "      <th>dropoff_centroid_longitude</th>\n",
       "      <th>dropoff_centroid_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3485f158f4e4151a50a3666701f1d6480712e344</td>\n",
       "      <td>8dd990653cc2793d96b80ae45e928afc9a590b21b491ee...</td>\n",
       "      <td>2018-04-26 10:15:00</td>\n",
       "      <td>2018-04-26 10:15:00</td>\n",
       "      <td>300</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.703108e+10</td>\n",
       "      <td>1.703108e+10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>Cash</td>\n",
       "      <td>KOAM Taxi Association</td>\n",
       "      <td>41.890922</td>\n",
       "      <td>-87.618868</td>\n",
       "      <td>POINT (-87.6188683546 41.8909220259)</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c9ea510fdf7256077b5f33daa9f3362b631a19d6</td>\n",
       "      <td>d59a55fcbd9ef11987cadd1a2dc93149edace06e23c37a...</td>\n",
       "      <td>2018-04-20 14:30:00</td>\n",
       "      <td>2018-04-20 14:30:00</td>\n",
       "      <td>420</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.703108e+10</td>\n",
       "      <td>1.703108e+10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.898332</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "      <td>41.892508</td>\n",
       "      <td>-87.626215</td>\n",
       "      <td>POINT (-87.6262149064 41.8925077809)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e0203090822169816112bb888a0bf20de23f1705</td>\n",
       "      <td>78a7a28fe001257044d5095fcb1ff7156a47411ff93cc5...</td>\n",
       "      <td>2018-04-08 01:00:00</td>\n",
       "      <td>2018-04-08 01:15:00</td>\n",
       "      <td>780</td>\n",
       "      <td>5.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.878866</td>\n",
       "      <td>-87.625192</td>\n",
       "      <td>POINT (-87.6251921424 41.8788655841)</td>\n",
       "      <td>41.944227</td>\n",
       "      <td>-87.655998</td>\n",
       "      <td>POINT (-87.6559981815 41.9442266014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dc27a74dce2a3fea125264863be02fea6e7e276a</td>\n",
       "      <td>c0250f358cae01c5319aeb7b39827e53f9a2259eb32e4c...</td>\n",
       "      <td>2018-04-19 12:15:00</td>\n",
       "      <td>2018-04-19 12:15:00</td>\n",
       "      <td>540</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.703132e+10</td>\n",
       "      <td>1.703108e+10</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Blue Ribbon Taxi Association Inc.</td>\n",
       "      <td>41.884987</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "      <td>41.893216</td>\n",
       "      <td>-87.637844</td>\n",
       "      <td>POINT (-87.6378442095 41.8932163595)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4549d2d605d9563a7e0e50c4b9d0b3336eb8cd4a</td>\n",
       "      <td>135f786cdda9db8da848725a7af2922b6886fbd43c609a...</td>\n",
       "      <td>2018-04-09 07:45:00</td>\n",
       "      <td>2018-04-09 08:45:00</td>\n",
       "      <td>3720</td>\n",
       "      <td>15.5</td>\n",
       "      <td>1.703107e+10</td>\n",
       "      <td>1.703198e+10</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.25</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.922083</td>\n",
       "      <td>-87.634156</td>\n",
       "      <td>POINT (-87.6341560931 41.922082541)</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    trip_id  \\\n",
       "0  3485f158f4e4151a50a3666701f1d6480712e344   \n",
       "1  c9ea510fdf7256077b5f33daa9f3362b631a19d6   \n",
       "2  e0203090822169816112bb888a0bf20de23f1705   \n",
       "3  dc27a74dce2a3fea125264863be02fea6e7e276a   \n",
       "4  4549d2d605d9563a7e0e50c4b9d0b3336eb8cd4a   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  8dd990653cc2793d96b80ae45e928afc9a590b21b491ee...  2018-04-26 10:15:00   \n",
       "1  d59a55fcbd9ef11987cadd1a2dc93149edace06e23c37a...  2018-04-20 14:30:00   \n",
       "2  78a7a28fe001257044d5095fcb1ff7156a47411ff93cc5...  2018-04-08 01:00:00   \n",
       "3  c0250f358cae01c5319aeb7b39827e53f9a2259eb32e4c...  2018-04-19 12:15:00   \n",
       "4  135f786cdda9db8da848725a7af2922b6886fbd43c609a...  2018-04-09 07:45:00   \n",
       "\n",
       "   trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0 2018-04-26 10:15:00           300         0.7         1.703108e+10   \n",
       "1 2018-04-20 14:30:00           420         0.7         1.703108e+10   \n",
       "2 2018-04-08 01:15:00           780         5.7                  NaN   \n",
       "3 2018-04-19 12:15:00           540         0.1         1.703132e+10   \n",
       "4 2018-04-09 08:45:00          3720        15.5         1.703107e+10   \n",
       "\n",
       "   dropoff_census_tract  pickup_community_area  dropoff_community_area  ...  \\\n",
       "0          1.703108e+10                      8                       8  ...   \n",
       "1          1.703108e+10                      8                       8  ...   \n",
       "2                   NaN                     32                       6  ...   \n",
       "3          1.703108e+10                     32                       8  ...   \n",
       "4          1.703198e+10                      7                      76  ...   \n",
       "\n",
       "   extras  trip_total  payment_type                            company  \\\n",
       "0     0.0        5.25          Cash              KOAM Taxi Association   \n",
       "1     0.0        6.00          Cash          Taxi Affiliation Services   \n",
       "2     1.0       17.75          Cash          Star North Management LLC   \n",
       "3     0.0       10.00   Credit Card  Blue Ribbon Taxi Association Inc.   \n",
       "4     0.0       43.25          Cash          Star North Management LLC   \n",
       "\n",
       "   pickup_centroid_latitude pickup_centroid_longitude  \\\n",
       "0                 41.890922                -87.618868   \n",
       "1                 41.898332                -87.620763   \n",
       "2                 41.878866                -87.625192   \n",
       "3                 41.884987                -87.620993   \n",
       "4                 41.922083                -87.634156   \n",
       "\n",
       "               pickup_centroid_location  dropoff_centroid_latitude  \\\n",
       "0  POINT (-87.6188683546 41.8909220259)                  41.892042   \n",
       "1  POINT (-87.6207628651 41.8983317935)                  41.892508   \n",
       "2  POINT (-87.6251921424 41.8788655841)                  41.944227   \n",
       "3  POINT (-87.6209929134 41.8849871918)                  41.893216   \n",
       "4   POINT (-87.6341560931 41.922082541)                  41.979071   \n",
       "\n",
       "   dropoff_centroid_longitude             dropoff_centroid_location  \n",
       "0                  -87.631864  POINT (-87.6318639497 41.8920421365)  \n",
       "1                  -87.626215  POINT (-87.6262149064 41.8925077809)  \n",
       "2                  -87.655998  POINT (-87.6559981815 41.9442266014)  \n",
       "3                  -87.637844  POINT (-87.6378442095 41.8932163595)  \n",
       "4                  -87.903040  POINT (-87.9030396611 41.9790708201)  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_trips.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1705e0ae5c848dfb4f00383177f0254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_total_rows(df):\n",
    "    \"\"\"\n",
    "    Count the number of all records.\n",
    "    \"\"\" \n",
    "    count_rows = df.count()\n",
    "    print ('Total data count is '+str(count_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data count is 38282870\n"
     ]
    }
   ],
   "source": [
    "check_total_rows(taxi_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema of the dataset look like the following.\n",
    "This matches the schema listed on the official website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- taxi_id: string (nullable = true)\n",
      " |-- trip_start_timestamp: timestamp (nullable = true)\n",
      " |-- trip_end_timestamp: timestamp (nullable = true)\n",
      " |-- trip_seconds: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- pickup_census_tract: long (nullable = true)\n",
      " |-- dropoff_census_tract: long (nullable = true)\n",
      " |-- pickup_community_area: integer (nullable = true)\n",
      " |-- dropoff_community_area: integer (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- extras: double (nullable = true)\n",
      " |-- trip_total: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- pickup_centroid_latitude: double (nullable = true)\n",
      " |-- pickup_centroid_longitude: double (nullable = true)\n",
      " |-- pickup_centroid_location: string (nullable = true)\n",
      " |-- dropoff_centroid_latitude: double (nullable = true)\n",
      " |-- dropoff_centroid_longitude: double (nullable = true)\n",
      " |-- dropoff_centroid_location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'taxi_id',\n",
       " 'trip_start_timestamp',\n",
       " 'trip_end_timestamp',\n",
       " 'trip_seconds',\n",
       " 'trip_miles',\n",
       " 'pickup_census_tract',\n",
       " 'dropoff_census_tract',\n",
       " 'pickup_community_area',\n",
       " 'dropoff_community_area',\n",
       " 'fare',\n",
       " 'tips',\n",
       " 'tolls',\n",
       " 'extras',\n",
       " 'trip_total',\n",
       " 'payment_type',\n",
       " 'company',\n",
       " 'pickup_centroid_latitude',\n",
       " 'pickup_centroid_longitude',\n",
       " 'pickup_centroid_location',\n",
       " 'dropoff_centroid_latitude',\n",
       " 'dropoff_centroid_longitude',\n",
       " 'dropoff_centroid_location']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset contains the follwong columns\n",
    "taxi_trips.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455429b15edc4547af862bfdd59179e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The check for missing values need to be performed\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "def check_missing_data(df, num_rows=1000 ,columns=None):\n",
    "    \"\"\"\n",
    "    Prints number of missing data fields found for every column.\n",
    "    If no set of columns specified, all columns checked\n",
    "    \"\"\"\n",
    " \n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "        \n",
    "    for col in columns:\n",
    "        count = df.select(col).limit(num_rows).withColumn('isNull_c',F.col(col).isNull()).where('isNull_c = True').count()\n",
    "        print(\"Column %s contains missing data: %s\" % (col, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column trip_id contains missing data: 0\n",
      "Column taxi_id contains missing data: 0\n",
      "Column trip_start_timestamp contains missing data: 0\n",
      "Column trip_end_timestamp contains missing data: 0\n",
      "Column trip_seconds contains missing data: 0\n",
      "Column trip_miles contains missing data: 0\n",
      "Column pickup_census_tract contains missing data: 314\n",
      "Column dropoff_census_tract contains missing data: 323\n",
      "Column pickup_community_area contains missing data: 38\n",
      "Column dropoff_community_area contains missing data: 65\n",
      "Column fare contains missing data: 0\n",
      "Column tips contains missing data: 0\n",
      "Column tolls contains missing data: 0\n",
      "Column extras contains missing data: 0\n",
      "Column trip_total contains missing data: 0\n",
      "Column payment_type contains missing data: 0\n",
      "Column company contains missing data: 0\n",
      "Column pickup_centroid_latitude contains missing data: 38\n",
      "Column pickup_centroid_longitude contains missing data: 38\n",
      "Column pickup_centroid_location contains missing data: 38\n",
      "Column dropoff_centroid_latitude contains missing data: 58\n",
      "Column dropoff_centroid_longitude contains missing data: 58\n",
      "Column dropoff_centroid_location contains missing data: 58\n"
     ]
    }
   ],
   "source": [
    "check_missing_data(taxi_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated on the official website, `Census Tract` is not shown for some trips. Or column can be blank if trips are outside of Chicago.\n",
    "<br> Some of the coordinate data is missing, and this should be decided where to be preserved or to be cleaned.\n",
    "<br> From random sample of 1000 rows, we can observe that dataset has at least 10% of rows with missing data.\n",
    "\n",
    "For analysis this can be not very crucial, but for Machine Learning jobs this data may become vital.\n",
    "\n",
    "We use different schemas for different departments later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for column trip_id\n",
      "+-------+--------------------+\n",
      "|summary|             trip_id|\n",
      "+-------+--------------------+\n",
      "|  count|                1000|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|006e531f6de2d7022...|\n",
      "|    max|ff2868907eaf6633b...|\n",
      "+-------+--------------------+\n",
      "\n",
      "Summary for column taxi_id\n",
      "+-------+--------------------+\n",
      "|summary|             taxi_id|\n",
      "+-------+--------------------+\n",
      "|  count|                1000|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|008ca9f6e7dff925f...|\n",
      "|    max|fe3f654450472e6aa...|\n",
      "+-------+--------------------+\n",
      "\n",
      "Summary for column trip_start_timestamp\n",
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n",
      "Summary for column trip_end_timestamp\n",
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n",
      "Summary for column trip_seconds\n",
      "+-------+-----------------+\n",
      "|summary|     trip_seconds|\n",
      "+-------+-----------------+\n",
      "|  count|             1000|\n",
      "|   mean|          813.307|\n",
      "| stddev|750.9443509136808|\n",
      "|    min|                0|\n",
      "|    max|             4582|\n",
      "+-------+-----------------+\n",
      "\n",
      "Summary for column trip_miles\n",
      "+-------+------------------+\n",
      "|summary|        trip_miles|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|2.9749999999999948|\n",
      "| stddev| 5.401883629508021|\n",
      "|    min|               0.0|\n",
      "|    max|             37.15|\n",
      "+-------+------------------+\n",
      "\n",
      "Summary for column pickup_census_tract\n",
      "+-------+-------------------+\n",
      "|summary|pickup_census_tract|\n",
      "+-------+-------------------+\n",
      "|  count|                686|\n",
      "|   mean|1.70313877268207E10|\n",
      "| stddev| 350825.81531589996|\n",
      "|    min|        17031032100|\n",
      "|    max|        17031980100|\n",
      "+-------+-------------------+\n",
      "\n",
      "Summary for column dropoff_census_tract\n",
      "+-------+--------------------+\n",
      "|summary|dropoff_census_tract|\n",
      "+-------+--------------------+\n",
      "|  count|                 677|\n",
      "|   mean|1.703136321007533...|\n",
      "| stddev|   335847.0741920357|\n",
      "|    min|         17031010300|\n",
      "|    max|         17031980100|\n",
      "+-------+--------------------+\n",
      "\n",
      "Summary for column pickup_community_area\n",
      "+-------+---------------------+\n",
      "|summary|pickup_community_area|\n",
      "+-------+---------------------+\n",
      "|  count|                  962|\n",
      "|   mean|   24.907484407484407|\n",
      "| stddev|    20.87539879452993|\n",
      "|    min|                    1|\n",
      "|    max|                   77|\n",
      "+-------+---------------------+\n",
      "\n",
      "Summary for column dropoff_community_area\n",
      "+-------+----------------------+\n",
      "|summary|dropoff_community_area|\n",
      "+-------+----------------------+\n",
      "|  count|                   935|\n",
      "|   mean|     22.21390374331551|\n",
      "| stddev|    18.219904029349074|\n",
      "|    min|                     1|\n",
      "|    max|                    77|\n",
      "+-------+----------------------+\n",
      "\n",
      "Summary for column fare\n",
      "+-------+------------------+\n",
      "|summary|              fare|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|14.265859999999998|\n",
      "| stddev| 16.99725606602943|\n",
      "|    min|              3.25|\n",
      "|    max|             351.0|\n",
      "+-------+------------------+\n",
      "\n",
      "Summary for column tips\n",
      "+-------+------------------+\n",
      "|summary|              tips|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean|1.6904799999999993|\n",
      "| stddev|2.8684781219901834|\n",
      "|    min|               0.0|\n",
      "|    max|              20.0|\n",
      "+-------+------------------+\n",
      "\n",
      "Summary for column tolls\n",
      "+-------+-----------------+\n",
      "|summary|            tolls|\n",
      "+-------+-----------------+\n",
      "|  count|             1000|\n",
      "|   mean|            0.023|\n",
      "| stddev|0.727323861838727|\n",
      "|    min|              0.0|\n",
      "|    max|             23.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "Summary for column extras\n",
      "+-------+-----------------+\n",
      "|summary|           extras|\n",
      "+-------+-----------------+\n",
      "|  count|             1000|\n",
      "|   mean|          1.07612|\n",
      "| stddev|3.171829227156823|\n",
      "|    min|              0.0|\n",
      "|    max|             57.0|\n",
      "+-------+-----------------+\n",
      "\n",
      "Summary for column trip_total\n",
      "+-------+------------------+\n",
      "|summary|        trip_total|\n",
      "+-------+------------------+\n",
      "|  count|              1000|\n",
      "|   mean| 17.08795999999999|\n",
      "| stddev|19.920275339663554|\n",
      "|    min|              3.25|\n",
      "|    max|             353.0|\n",
      "+-------+------------------+\n",
      "\n",
      "Summary for column payment_type\n",
      "+-------+------------+\n",
      "|summary|payment_type|\n",
      "+-------+------------+\n",
      "|  count|        1000|\n",
      "|   mean|        null|\n",
      "| stddev|        null|\n",
      "|    min|        Cash|\n",
      "|    max|     Unknown|\n",
      "+-------+------------+\n",
      "\n",
      "Summary for column company\n",
      "+-------+--------------------+\n",
      "|summary|             company|\n",
      "+-------+--------------------+\n",
      "|  count|                1000|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|1469 - 64126 Omar...|\n",
      "|    max| Top Cab Affiliation|\n",
      "+-------+--------------------+\n",
      "\n",
      "Summary for column pickup_centroid_latitude\n",
      "+-------+------------------------+\n",
      "|summary|pickup_centroid_latitude|\n",
      "+-------+------------------------+\n",
      "|  count|                     962|\n",
      "|   mean|      41.899944959555235|\n",
      "| stddev|     0.03984631081193228|\n",
      "|    min|            41.694878966|\n",
      "|    max|            42.009622881|\n",
      "+-------+------------------------+\n",
      "\n",
      "Summary for column pickup_centroid_longitude\n",
      "+-------+-------------------------+\n",
      "|summary|pickup_centroid_longitude|\n",
      "+-------+-------------------------+\n",
      "|  count|                      962|\n",
      "|   mean|       -87.66226300133061|\n",
      "| stddev|      0.07878048563025088|\n",
      "|    min|            -87.913624596|\n",
      "|    max|            -87.583143717|\n",
      "+-------+-------------------------+\n",
      "\n",
      "Summary for column pickup_centroid_location\n",
      "+-------+------------------------+\n",
      "|summary|pickup_centroid_location|\n",
      "+-------+------------------------+\n",
      "|  count|                     962|\n",
      "|   mean|                    null|\n",
      "| stddev|                    null|\n",
      "|    min|    POINT (-87.583143...|\n",
      "|    max|    POINT (-87.913624...|\n",
      "+-------+------------------------+\n",
      "\n",
      "Summary for column dropoff_centroid_latitude\n",
      "+-------+-------------------------+\n",
      "|summary|dropoff_centroid_latitude|\n",
      "+-------+-------------------------+\n",
      "|  count|                      942|\n",
      "|   mean|        41.90111310780474|\n",
      "| stddev|      0.03811716450985905|\n",
      "|    min|             41.740205756|\n",
      "|    max|             42.015934376|\n",
      "+-------+-------------------------+\n",
      "\n",
      "Summary for column dropoff_centroid_longitude\n",
      "+-------+--------------------------+\n",
      "|summary|dropoff_centroid_longitude|\n",
      "+-------+--------------------------+\n",
      "|  count|                       942|\n",
      "|   mean|        -87.65565308000951|\n",
      "| stddev|         0.063736662482064|\n",
      "|    min|             -87.913624596|\n",
      "|    max|             -87.583143717|\n",
      "+-------+--------------------------+\n",
      "\n",
      "Summary for column dropoff_centroid_location\n",
      "+-------+-------------------------+\n",
      "|summary|dropoff_centroid_location|\n",
      "+-------+-------------------------+\n",
      "|  count|                      942|\n",
      "|   mean|                     null|\n",
      "| stddev|                     null|\n",
      "|    min|     POINT (-87.583143...|\n",
      "|    max|     POINT (-87.913624...|\n",
      "+-------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the summary of all columns\n",
    "def show_summary(df, num_rows=1000):\n",
    "    \"\"\"\n",
    "    Shows summary data (min, max, average, etc.) for each column\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        print(\"Summary for column %s\" % col)\n",
    "        taxi_trips.limit(num_rows).select(*[col]).describe().show()\n",
    "\n",
    "show_summary(taxi_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us show unique companies that work in Chicago\n",
    "There is also a sepatate datset that is collected by Uber and Lyft companies,\n",
    "that only contains trip information, withoud taxi_id, company_name columns.\n",
    "\n",
    "This data can be also used to for enrichment of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3556 - 36214 RC Andrews Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Service Taxi Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metro Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicago Taxicab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FlashCab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4053 - 40193 Adwar H. Nikola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>American United Taxi Affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5 Star Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5006 - 39261 Salifu Bawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0694 - 59280 Chinesco Trans Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>American United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Suburban Dispatch LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>U Taxicab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Flash Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Nova Taxi Affiliation Llc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Checker Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Choice Taxi Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Chicago Medallion Leasing INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Checker Taxi Affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3591 - 63480 Chuks Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>T.A.S. - Payment Only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2241 - 44667 - Felman Corp, Manuel Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Setare Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dispatch Taxi Affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Taxi Affiliation Service Yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sun Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3721 - Santamaria Express, Alvaro Santamaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Patriot Taxi Dba Peace Taxi Associat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Blue Ribbon Taxi Association Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3623 - 72222 Arrington Enterprises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Norshore Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Yellow Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4615 - 83503 Tyrone Henderson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Chicago Star Taxicab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>24 Seven Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2092 - 61288 Sbeih company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5997 - 65283 AW Services Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>312 Medallion Management Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3620 - 52292 David K. Cab Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Leonard Cab Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>KOAM Taxi Association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>303 Taxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5874 - 73628 Sergey Cab Corp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4787 - 56058 Reny Cab Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2733 - 74600 Benny Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1469 - 64126 Omar Jada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4623 - 27290 Jay Kim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Star North Management LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Arro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6743 - 78771 Luhak Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Chicago Independents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>6057 - 24657 Richard Addo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Chicago Medallion Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3669 - 85800 Jordan Taxi Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5074 - 54002 Ahzmi Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>City Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2192 - 73487 Zeymane Corp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         company\n",
       "0    3556 - 36214 RC Andrews Cab\n",
       "1       Service Taxi Association\n",
       "2                    Metro Group\n",
       "3                Chicago Taxicab\n",
       "4                       FlashCab\n",
       "..                           ...\n",
       "71  Chicago Medallion Management\n",
       "72  3669 - 85800 Jordan Taxi Inc\n",
       "73        5074 - 54002 Ahzmi Inc\n",
       "74                  City Service\n",
       "75     2192 - 73487 Zeymane Corp\n",
       "\n",
       "[76 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_trips.select(\"company\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we check how many taxi Cabs are registered in Chicago at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6645 cabs registered in Chicago city at the moment\n"
     ]
    }
   ],
   "source": [
    "number_of_cabs = taxi_trips.select(\"taxi_id\").distinct().count()\n",
    "print(\"There are %s cabs registered in Chicago city at the moment\" % number_of_cabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2a: Loading Temperature Data\n",
    "\n",
    "Temperature data in Chicago is a dataset of temperatures recoded for each day at  **CHICAGO OHARE INTERNATIONAL AIRPORT, IL US** station since 1954.\n",
    "And this perfecly fits our requirements to gather weather conditions for each day in Chicago City.\n",
    "\n",
    "The weather dataset can be found here: https://www.ncdc.noaa.gov/cdo-web/datasets\n",
    "\n",
    "We will use a subset part of this dataset for the period from january 2018 till today, as we interested about most recent events.\n",
    "\n",
    "The dataset is quite small and can be easily stored locally.\n",
    "\n",
    "Weather data for one day looks like the following:\n",
    "![Weather](./assets/Chicago_weather_sample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c118a14a1064a9ead5d8282320bb4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2a: Loading Temperature Data in Chicago for the period from january 2018 till today\n",
    "weather_df = spark.read.option(\"header\", \"true\").option(\"inferschema\", \"true\").csv(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>AWND</th>\n",
       "      <th>AWND_ATTRIBUTES</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNOW_ATTRIBUTES</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>SNWD_ATTRIBUTES</th>\n",
       "      <th>...</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT04_ATTRIBUTES</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT05_ATTRIBUTES</th>\n",
       "      <th>WT06</th>\n",
       "      <th>WT06_ATTRIBUTES</th>\n",
       "      <th>WT08</th>\n",
       "      <th>WT08_ATTRIBUTES</th>\n",
       "      <th>WT09</th>\n",
       "      <th>WT09_ATTRIBUTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USW00094846</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4.8</td>\n",
       "      <td>,,W</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>30.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USW00094846</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>30.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USW00094846</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5.5</td>\n",
       "      <td>,,W</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T,,W,2400</td>\n",
       "      <td>3.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>30.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USW00094846</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5.5</td>\n",
       "      <td>,,W</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>30.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USW00094846</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>4.8</td>\n",
       "      <td>,,W</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>,,W</td>\n",
       "      <td>30.0</td>\n",
       "      <td>,,W,2400</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION       DATE  AWND AWND_ATTRIBUTES  PRCP PRCP_ATTRIBUTES  SNOW  \\\n",
       "0  USW00094846 2018-01-01   4.8             ,,W   0.0        ,,W,2400   0.0   \n",
       "1  USW00094846 2018-01-02   5.0             ,,W   0.0        ,,W,2400   0.0   \n",
       "2  USW00094846 2018-01-03   5.5             ,,W   0.0       T,,W,2400   3.0   \n",
       "3  USW00094846 2018-01-04   5.5             ,,W   0.0        ,,W,2400   0.0   \n",
       "4  USW00094846 2018-01-05   4.8             ,,W   0.0        ,,W,2400   0.0   \n",
       "\n",
       "  SNOW_ATTRIBUTES  SNWD SNWD_ATTRIBUTES  ...  WT04 WT04_ATTRIBUTES  WT05  \\\n",
       "0             ,,W  30.0        ,,W,2400  ...  None            None  None   \n",
       "1             ,,W  30.0        ,,W,2400  ...  None            None  None   \n",
       "2             ,,W  30.0        ,,W,2400  ...  None            None  None   \n",
       "3             ,,W  30.0        ,,W,2400  ...  None            None  None   \n",
       "4             ,,W  30.0        ,,W,2400  ...  None            None  None   \n",
       "\n",
       "  WT05_ATTRIBUTES  WT06 WT06_ATTRIBUTES  WT08 WT08_ATTRIBUTES  WT09  \\\n",
       "0            None  None            None   NaN            None  None   \n",
       "1            None  None            None   NaN            None  None   \n",
       "2            None  None            None   1.0             ,,W  None   \n",
       "3            None  None            None   NaN            None  None   \n",
       "4            None  None            None   NaN            None  None   \n",
       "\n",
       "  WT09_ATTRIBUTES  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: string (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- AWND: double (nullable = true)\n",
      " |-- AWND_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNOW: double (nullable = true)\n",
      " |-- SNOW_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNWD: double (nullable = true)\n",
      " |-- SNWD_ATTRIBUTES: string (nullable = true)\n",
      " |-- TAVG: double (nullable = true)\n",
      " |-- TAVG_ATTRIBUTES: string (nullable = true)\n",
      " |-- TMAX: double (nullable = true)\n",
      " |-- TMAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- TMIN: double (nullable = true)\n",
      " |-- TMIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- WDF2: double (nullable = true)\n",
      " |-- WDF2_ATTRIBUTES: string (nullable = true)\n",
      " |-- WDF5: double (nullable = true)\n",
      " |-- WDF5_ATTRIBUTES: string (nullable = true)\n",
      " |-- WSF2: double (nullable = true)\n",
      " |-- WSF2_ATTRIBUTES: string (nullable = true)\n",
      " |-- WSF5: double (nullable = true)\n",
      " |-- WSF5_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT01: double (nullable = true)\n",
      " |-- WT01_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT02: double (nullable = true)\n",
      " |-- WT02_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT03: double (nullable = true)\n",
      " |-- WT03_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT04: double (nullable = true)\n",
      " |-- WT04_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT05: double (nullable = true)\n",
      " |-- WT05_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT06: double (nullable = true)\n",
      " |-- WT06_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT08: double (nullable = true)\n",
      " |-- WT08_ATTRIBUTES: string (nullable = true)\n",
      " |-- WT09: double (nullable = true)\n",
      " |-- WT09_ATTRIBUTES: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains too many columns, and we would like to use only the data about weather conditions and temperature of each day only.\n",
    "<br>This may help us to predict taxi demand, price, and possible delays due to heavy traffic, etc.\n",
    "<br>We would need to preprocess this data for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column STATION contains missing data: 0\n",
      "Column DATE contains missing data: 0\n",
      "Column AWND contains missing data: 2\n",
      "Column AWND_ATTRIBUTES contains missing data: 2\n",
      "Column PRCP contains missing data: 1\n",
      "Column PRCP_ATTRIBUTES contains missing data: 1\n",
      "Column SNOW contains missing data: 1\n",
      "Column SNOW_ATTRIBUTES contains missing data: 1\n",
      "Column SNWD contains missing data: 1\n",
      "Column SNWD_ATTRIBUTES contains missing data: 1\n",
      "Column TAVG contains missing data: 0\n",
      "Column TAVG_ATTRIBUTES contains missing data: 0\n",
      "Column TMAX contains missing data: 1\n",
      "Column TMAX_ATTRIBUTES contains missing data: 1\n",
      "Column TMIN contains missing data: 1\n",
      "Column TMIN_ATTRIBUTES contains missing data: 1\n",
      "Column WDF2 contains missing data: 2\n",
      "Column WDF2_ATTRIBUTES contains missing data: 2\n",
      "Column WDF5 contains missing data: 3\n",
      "Column WDF5_ATTRIBUTES contains missing data: 3\n",
      "Column WSF2 contains missing data: 2\n",
      "Column WSF2_ATTRIBUTES contains missing data: 2\n",
      "Column WSF5 contains missing data: 3\n",
      "Column WSF5_ATTRIBUTES contains missing data: 3\n",
      "Column WT01 contains missing data: 402\n",
      "Column WT01_ATTRIBUTES contains missing data: 402\n",
      "Column WT02 contains missing data: 749\n",
      "Column WT02_ATTRIBUTES contains missing data: 749\n",
      "Column WT03 contains missing data: 671\n",
      "Column WT03_ATTRIBUTES contains missing data: 671\n",
      "Column WT04 contains missing data: 767\n",
      "Column WT04_ATTRIBUTES contains missing data: 767\n",
      "Column WT05 contains missing data: 777\n",
      "Column WT05_ATTRIBUTES contains missing data: 777\n",
      "Column WT06 contains missing data: 756\n",
      "Column WT06_ATTRIBUTES contains missing data: 756\n",
      "Column WT08 contains missing data: 611\n",
      "Column WT08_ATTRIBUTES contains missing data: 611\n",
      "Column WT09 contains missing data: 766\n",
      "Column WT09_ATTRIBUTES contains missing data: 766\n"
     ]
    }
   ],
   "source": [
    "check_missing_data(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** Missing data in this dataset is expected. As many columns contain conditions data with `1` - when condition occured and `null` when conditions were normal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data count is 779\n"
     ]
    }
   ],
   "source": [
    "check_total_rows(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The conceptual data model is going to extend existing Taxi Trips Dataset with Weather temperature collected for each given day in Chicago Ohara Airport. The weather data additionally contains weather conditions, that may be useful for prediction of various traveling patterns in Chicago city.\n",
    "\n",
    "Initial Taxi Trips dataset contains **23** columns and weather dataset - **40** columns.\n",
    "\n",
    "Given the combined data, we would be able to provide enough data that can be used for many use cases.\n",
    "\n",
    "\n",
    "The following use-cases using created data models can be targeted:\n",
    "\n",
    "* as a Taxi Driver, I would be able to choose better location for passenger pickups, during exact hour of the day, season, and weather conditions at the given moment.\n",
    "\n",
    "* as a Data Analyst, I would be able to analyze data for each Trip, for each driver. I would be able to collect driver ratings based on number of trips per week/month and amount of tips collected.\n",
    "\n",
    "* as a Machine Learning engineer, I would be able to use data to predict best places for passenger pickups, if tips will be given, if the passenger flow will increase at any given day, etc.\n",
    "\n",
    "* Additionally, information that is collected for each Taxi Cab can help companies to decide if any repair works should be performed\n",
    "\n",
    "* Payment patterns can be defined, e.g for any short/long trips\n",
    "\n",
    "* Can ride-sharing opportunity be explored and in which area with with higher probability\n",
    "\n",
    "\n",
    "And many other use cases can be supported.\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The weather data has been already downloaded for the period of our interest.\n",
    "Dataset is quite small, as we are going to target information for period from 2018 until 21 February 2020.\n",
    "\n",
    "The Taxi Data is also prefiltered to contain only data for period of our interest. We will take into account part of the dataset of around 39.000.000 rows.\n",
    "\n",
    "* After data is loaded, we ensure that dataframes are valid. Checking the validity of schema have been done in Section 1.\n",
    "* First, we would need to convert date and time column to DateType.\n",
    "* For ease of joins between tables we extract information about day, year, hour, etc. This would allow us to easily join dataframes on time periods.\n",
    "* Next, each taxi trip will be enriched with this data\n",
    "* Community area data is extracted next. Each location coordinate point will be rounded to precision of 15 meters. thus allowing us to collect most popular passanger point in Chicago City.\n",
    "* Extraction of data for each taxi cab is performed next. This data will contain a lot of useful information for any department and can be used to create helpful API for drivers, of passenders and help them to estimate cost for the ride.\n",
    "* During the next step, we process the weather data and split it into two tables that should be used separately.\n",
    "* Weather condition data is a helpful datset that can be ysed to enrich your data `on-demand`\n",
    "* The main taxi dataset will be stored with hidden locations data included. As this may include a lot of insightful information later on. We simply cannot throw our 30% of such data.\n",
    "* For wide range of analytic and `ad-hoc` use cases, we store preprocessed taxi trips dataset with no missing values present in them in a separate table.\n",
    "* Finally, the main fact table will be created. The preprocessed taxi trips dataset with no missing values will be joined with weather data.\n",
    "\n",
    "Next section will contain step by step guide through the ETL pipeline in much more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run Pipelines to Model the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4a. Taxi Data ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data checks\n",
    "Before each step we perform data check whether dataframes are valid.\n",
    "However, one need to note here, that if full data is used ( more that 100 Gb) these check may drastically slow down the whole pipeline.\n",
    "And should be performed for final tests only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1378861430374ef38366983a5766ea66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_dataframe_correctness(df):\n",
    "    check = df.select(*[df.columns]).limit(5)\n",
    "    if check.count() <= 0:\n",
    "        raise ValueError(\"Dataframe cannot be empty. Check if Spark Session is still running\")\n",
    "    else:\n",
    "        print(\"Dataframe is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe is available\n",
      "Dataframe is available\n"
     ]
    }
   ],
   "source": [
    "check_dataframe_correctness(taxi_trips)\n",
    "check_dataframe_correctness(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extract any useful information from timestamp field and join this data with *Taxi Data*.\n",
    "<br>We will use this data for easier joining with *Weather Data* on **(year, dayofyear)** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b230bc573a71428bb513df039d768485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "col = taxi_trips.trip_start_timestamp\n",
    "time_df = taxi_trips.select(year(col).alias('year'), month(col).alias('month'), dayofmonth(col).alias('day'), dayofyear(col).alias('dayofyear'), hour(col).alias('hour'), minute(col).alias('min'), weekofyear(col).alias('week_no'), unix_timestamp(col).alias('unix_ts'),\"trip_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>week_no</th>\n",
       "      <th>unix_ts</th>\n",
       "      <th>trip_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>116</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>1524737700</td>\n",
       "      <td>3485f158f4e4151a50a3666701f1d6480712e344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>1524234600</td>\n",
       "      <td>c9ea510fdf7256077b5f33daa9f3362b631a19d6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1523149200</td>\n",
       "      <td>e0203090822169816112bb888a0bf20de23f1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>109</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>1524140100</td>\n",
       "      <td>dc27a74dce2a3fea125264863be02fea6e7e276a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>1523259900</td>\n",
       "      <td>4549d2d605d9563a7e0e50c4b9d0b3336eb8cd4a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  dayofyear  hour  min  week_no     unix_ts  \\\n",
       "0  2018      4   26        116    10   15       17  1524737700   \n",
       "1  2018      4   20        110    14   30       16  1524234600   \n",
       "2  2018      4    8         98     1    0       14  1523149200   \n",
       "3  2018      4   19        109    12   15       16  1524140100   \n",
       "4  2018      4    9         99     7   45       15  1523259900   \n",
       "\n",
       "                                    trip_id  \n",
       "0  3485f158f4e4151a50a3666701f1d6480712e344  \n",
       "1  c9ea510fdf7256077b5f33daa9f3362b631a19d6  \n",
       "2  e0203090822169816112bb888a0bf20de23f1705  \n",
       "3  dc27a74dce2a3fea125264863be02fea6e7e276a  \n",
       "4  4549d2d605d9563a7e0e50c4b9d0b3336eb8cd4a  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a116040921418ba746b4b974f1d44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will enrich main dataframe with extracted data\n",
    "taxi_trips = taxi_trips.join(time_df, on=\"trip_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'taxi_id',\n",
       " 'trip_start_timestamp',\n",
       " 'trip_end_timestamp',\n",
       " 'trip_seconds',\n",
       " 'trip_miles',\n",
       " 'pickup_census_tract',\n",
       " 'dropoff_census_tract',\n",
       " 'pickup_community_area',\n",
       " 'dropoff_community_area',\n",
       " 'fare',\n",
       " 'tips',\n",
       " 'tolls',\n",
       " 'extras',\n",
       " 'trip_total',\n",
       " 'payment_type',\n",
       " 'company',\n",
       " 'pickup_centroid_latitude',\n",
       " 'pickup_centroid_longitude',\n",
       " 'pickup_centroid_location',\n",
       " 'dropoff_centroid_latitude',\n",
       " 'dropoff_centroid_longitude',\n",
       " 'dropoff_centroid_location',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dayofyear',\n",
       " 'hour',\n",
       " 'min',\n",
       " 'week_no',\n",
       " 'unix_ts']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_trips.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Areas Table\n",
    "\n",
    "This table contains collection of coordinates in each community area.\n",
    "\n",
    "Coordinates can be easily plotted, or grouped to find the closest one to the taxi driver of to find closes taxi cab to the passenger.\n",
    "\n",
    "This table won't have unique PRIMARY Key and further this would need autoincrement id, if added to relational database.\n",
    "\n",
    "Nexy, we would need to round coordinate points,\n",
    "<br>as we need to know only location with precision up to 20-50 meters for most use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[community_area: int, centroid_latitude: double, centroid_longitude: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use round function here from pyspark.sql.functions\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "PRECISION = 4  #individual street precision, 15 meters\n",
    "community_areas_df = taxi_trips.select(\n",
    "                col(\"pickup_community_area\").alias(\"community_area\"), \n",
    "                round(taxi_trips[\"pickup_centroid_latitude\"],PRECISION).alias(\"centroid_latitude\"),\n",
    "                round(taxi_trips[\"pickup_centroid_longitude\"],PRECISION).alias(\"centroid_longitude\")\n",
    "            )\n",
    "community_areas_df.dropDuplicates(subset = [\"centroid_latitude\", \"centroid_longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community_area</th>\n",
       "      <th>centroid_latitude</th>\n",
       "      <th>centroid_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>41.8992</td>\n",
       "      <td>-87.6262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>41.8810</td>\n",
       "      <td>-87.6327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>41.8853</td>\n",
       "      <td>-87.6428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>41.8983</td>\n",
       "      <td>-87.6208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>41.9442</td>\n",
       "      <td>-87.6560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   community_area  centroid_latitude  centroid_longitude\n",
       "0               8            41.8992            -87.6262\n",
       "1              32            41.8810            -87.6327\n",
       "2              28            41.8853            -87.6428\n",
       "3               8            41.8983            -87.6208\n",
       "4               6            41.9442            -87.6560"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_areas_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabs Table\n",
    "\n",
    "Collect data relevant for each taxi cab. So data can be easily grouped and searched by standalone cab or driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_columns = ['trip_id', 'taxi_id',                          # taxi data\n",
    "                'trip_start_timestamp', 'trip_end_timestamp',  # trip time\n",
    "                'trip_seconds', 'trip_miles',                  # trip time\n",
    "                'fare', 'tips', 'tolls', 'extras',             # cost data\n",
    "                'trip_total', 'payment_type',                  # fare data\n",
    "                'company'                                      # company where taxi cab is registered\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_df = taxi_trips.select(*[cabs_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>fare</th>\n",
       "      <th>tips</th>\n",
       "      <th>tolls</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000040bde022bec832268c32707fd802c0a3d956</td>\n",
       "      <td>adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...</td>\n",
       "      <td>2018-06-14 10:00:00</td>\n",
       "      <td>2018-06-14 10:15:00</td>\n",
       "      <td>600</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.25</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b5c4c37a2c154b6f703803068625c67f81017</td>\n",
       "      <td>0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...</td>\n",
       "      <td>2019-09-15 00:00:00</td>\n",
       "      <td>2019-09-15 00:15:00</td>\n",
       "      <td>1020</td>\n",
       "      <td>0.10</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d50f70b61120f8342e039410a6ce077db84d2</td>\n",
       "      <td>839c8a07639d9477fe43f3e2b7be014a3deead1930460c...</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>478</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000f9ef866c0f120314ceb0d9290e71e7c23fd6a</td>\n",
       "      <td>e93472aad9e00c0a523e1a861ae897303548d713ba63d3...</td>\n",
       "      <td>2019-08-01 05:30:00</td>\n",
       "      <td>2019-08-01 06:00:00</td>\n",
       "      <td>1657</td>\n",
       "      <td>2.64</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.25</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Flash Cab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010ea18098781e411e7ea4f92e400e7058de94c</td>\n",
       "      <td>93930278eb7df9672cdc277c92e59dae0f82116868ce1c...</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>420</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>7.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    trip_id  \\\n",
       "0  000040bde022bec832268c32707fd802c0a3d956   \n",
       "1  000b5c4c37a2c154b6f703803068625c67f81017   \n",
       "2  000d50f70b61120f8342e039410a6ce077db84d2   \n",
       "3  000f9ef866c0f120314ceb0d9290e71e7c23fd6a   \n",
       "4  0010ea18098781e411e7ea4f92e400e7058de94c   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...  2018-06-14 10:00:00   \n",
       "1  0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...  2019-09-15 00:00:00   \n",
       "2  839c8a07639d9477fe43f3e2b7be014a3deead1930460c...  2019-08-24 23:45:00   \n",
       "3  e93472aad9e00c0a523e1a861ae897303548d713ba63d3...  2019-08-01 05:30:00   \n",
       "4  93930278eb7df9672cdc277c92e59dae0f82116868ce1c...  2018-06-15 20:45:00   \n",
       "\n",
       "   trip_end_timestamp  trip_seconds  trip_miles   fare  tips  tolls  extras  \\\n",
       "0 2018-06-14 10:15:00           600        0.80   7.25   3.0    0.0     0.0   \n",
       "1 2019-09-15 00:15:00          1020        0.10  11.00   0.0    0.0     0.0   \n",
       "2 2019-08-24 23:45:00           478        1.35   7.25   0.0    0.0     0.0   \n",
       "3 2019-08-01 06:00:00          1657        2.64  15.25   0.0    0.0     0.0   \n",
       "4 2018-06-15 20:45:00           420        0.00   6.25   0.0    0.0     1.5   \n",
       "\n",
       "   trip_total payment_type                    company  \n",
       "0       10.25  Credit Card  Star North Management LLC  \n",
       "1       11.00         Cash  Taxi Affiliation Services  \n",
       "2        7.25         Cash  Chicago Carriage Cab Corp  \n",
       "3       15.25         Cash                  Flash Cab  \n",
       "4        7.75         Cash  Taxi Affiliation Services  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How cabs data look like\n",
    "cabs_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Taxi Trips dataset\n",
    "\n",
    "This table will contain enriched Chicago taxi Dataset, and can be used for vast amount of use cases.\n",
    "\n",
    "Next, we drop the columns that have no relevant information and, in general, that contain duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_centroid_location</th>\n",
       "      <th>dropoff_centroid_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POINT (-87.642648998 41.8792550844)</td>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "      <td>POINT (-87.6317173661 41.9146162864)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POINT (-87.6317173661 41.9146162864)</td>\n",
       "      <td>POINT (-87.6308650266 41.9058577688)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup_centroid_location             dropoff_centroid_location\n",
       "0  POINT (-87.6327464887 41.8809944707)  POINT (-87.6209929134 41.8849871918)\n",
       "1   POINT (-87.642648998 41.8792550844)  POINT (-87.6207628651 41.8983317935)\n",
       "2  POINT (-87.6207628651 41.8983317935)  POINT (-87.6317173661 41.9146162864)\n",
       "3  POINT (-87.6207628651 41.8983317935)  POINT (-87.6209929134 41.8849871918)\n",
       "4  POINT (-87.6317173661 41.9146162864)  POINT (-87.6308650266 41.9058577688)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset has columns that contains POINT objects as string values. This can be filtered out for now\n",
    "\n",
    "taxi_trips.select(*[\"pickup_centroid_location\", \"dropoff_centroid_location\"]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>dropoff_centroid_longitude</th>\n",
       "      <th>dropoff_centroid_location</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>hour</th>\n",
       "      <th>min</th>\n",
       "      <th>week_no</th>\n",
       "      <th>unix_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000040bde022bec832268c32707fd802c0a3d956</td>\n",
       "      <td>adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...</td>\n",
       "      <td>2018-06-14 10:00:00</td>\n",
       "      <td>2018-06-14 10:15:00</td>\n",
       "      <td>600</td>\n",
       "      <td>0.80</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031320100</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1528970400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b5c4c37a2c154b6f703803068625c67f81017</td>\n",
       "      <td>0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...</td>\n",
       "      <td>2019-09-15 00:00:00</td>\n",
       "      <td>2019-09-15 00:15:00</td>\n",
       "      <td>1020</td>\n",
       "      <td>0.10</td>\n",
       "      <td>17031281900</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>POINT (-87.6207628651 41.8983317935)</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1568505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d50f70b61120f8342e039410a6ce077db84d2</td>\n",
       "      <td>839c8a07639d9477fe43f3e2b7be014a3deead1930460c...</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>478</td>\n",
       "      <td>1.35</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>17031071500</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.631717</td>\n",
       "      <td>POINT (-87.6317173661 41.9146162864)</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>34</td>\n",
       "      <td>1566690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000f9ef866c0f120314ceb0d9290e71e7c23fd6a</td>\n",
       "      <td>e93472aad9e00c0a523e1a861ae897303548d713ba63d3...</td>\n",
       "      <td>2019-08-01 05:30:00</td>\n",
       "      <td>2019-08-01 06:00:00</td>\n",
       "      <td>1657</td>\n",
       "      <td>2.64</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>17031320100</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>POINT (-87.6209929134 41.8849871918)</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>1564637400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010ea18098781e411e7ea4f92e400e7058de94c</td>\n",
       "      <td>93930278eb7df9672cdc277c92e59dae0f82116868ce1c...</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>420</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17031071500</td>\n",
       "      <td>17031080202</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.630865</td>\n",
       "      <td>POINT (-87.6308650266 41.9058577688)</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>166</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>24</td>\n",
       "      <td>1529095500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    trip_id  \\\n",
       "0  000040bde022bec832268c32707fd802c0a3d956   \n",
       "1  000b5c4c37a2c154b6f703803068625c67f81017   \n",
       "2  000d50f70b61120f8342e039410a6ce077db84d2   \n",
       "3  000f9ef866c0f120314ceb0d9290e71e7c23fd6a   \n",
       "4  0010ea18098781e411e7ea4f92e400e7058de94c   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...  2018-06-14 10:00:00   \n",
       "1  0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...  2019-09-15 00:00:00   \n",
       "2  839c8a07639d9477fe43f3e2b7be014a3deead1930460c...  2019-08-24 23:45:00   \n",
       "3  e93472aad9e00c0a523e1a861ae897303548d713ba63d3...  2019-08-01 05:30:00   \n",
       "4  93930278eb7df9672cdc277c92e59dae0f82116868ce1c...  2018-06-15 20:45:00   \n",
       "\n",
       "   trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0 2018-06-14 10:15:00           600        0.80          17031839100   \n",
       "1 2019-09-15 00:15:00          1020        0.10          17031281900   \n",
       "2 2019-08-24 23:45:00           478        1.35          17031081300   \n",
       "3 2019-08-01 06:00:00          1657        2.64          17031081300   \n",
       "4 2018-06-15 20:45:00           420        0.00          17031071500   \n",
       "\n",
       "   dropoff_census_tract  pickup_community_area  dropoff_community_area  ...  \\\n",
       "0           17031320100                     32                      32  ...   \n",
       "1           17031081300                     28                       8  ...   \n",
       "2           17031071500                      8                       7  ...   \n",
       "3           17031320100                      8                      32  ...   \n",
       "4           17031080202                      7                       8  ...   \n",
       "\n",
       "   dropoff_centroid_longitude             dropoff_centroid_location  year  \\\n",
       "0                  -87.620993  POINT (-87.6209929134 41.8849871918)  2018   \n",
       "1                  -87.620763  POINT (-87.6207628651 41.8983317935)  2019   \n",
       "2                  -87.631717  POINT (-87.6317173661 41.9146162864)  2019   \n",
       "3                  -87.620993  POINT (-87.6209929134 41.8849871918)  2019   \n",
       "4                  -87.630865  POINT (-87.6308650266 41.9058577688)  2018   \n",
       "\n",
       "   month  day dayofyear hour  min  week_no     unix_ts  \n",
       "0      6   14       165   10    0       24  1528970400  \n",
       "1      9   15       258    0    0       37  1568505600  \n",
       "2      8   24       236   23   45       34  1566690300  \n",
       "3      8    1       213    5   30       31  1564637400  \n",
       "4      6   15       166   20   45       24  1529095500  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_trips.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract these columns from all available columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425ea3edea544f448f32a9b5e8de67b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def diff(first, second):\n",
    "    \"\"\"\n",
    "    Function used to exclude `second` list from `first` list.\n",
    "    Resulting in reduced list.\n",
    "    \n",
    "    use-case: filtering out several colums from larger number of columns\n",
    "    \"\"\"\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac56941080747669cd6d5babcbc8302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trip_id', 'taxi_id', 'trip_start_timestamp', 'trip_end_timestamp', 'trip_seconds', 'trip_miles', 'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area', 'dropoff_community_area', 'fare', 'tips', 'tolls', 'extras', 'trip_total', 'payment_type', 'company', 'pickup_centroid_latitude', 'pickup_centroid_longitude', 'dropoff_centroid_latitude', 'dropoff_centroid_longitude', 'year', 'month', 'day', 'dayofyear', 'hour']"
     ]
    }
   ],
   "source": [
    "general_taxi_columns = diff(taxi_trips.columns, [\"pickup_centroid_location\", \"dropoff_centroid_location\", \"min\", \"week_no\", \"unix_ts\"])\n",
    "general_taxi_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1e0a9efebf40e09a6a21ccdfc7478a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out POINT columns, and columns that does not provide value to the data\n",
    "\n",
    "filtered_taxi_trips = taxi_trips.select(*general_taxi_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_centroid_latitude</th>\n",
       "      <th>pickup_centroid_longitude</th>\n",
       "      <th>dropoff_centroid_latitude</th>\n",
       "      <th>dropoff_centroid_longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000040bde022bec832268c32707fd802c0a3d956</td>\n",
       "      <td>adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...</td>\n",
       "      <td>2018-06-14 10:00:00</td>\n",
       "      <td>2018-06-14 10:15:00</td>\n",
       "      <td>600</td>\n",
       "      <td>0.80</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031320100</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>41.884987</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>165</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b5c4c37a2c154b6f703803068625c67f81017</td>\n",
       "      <td>0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...</td>\n",
       "      <td>2019-09-15 00:00:00</td>\n",
       "      <td>2019-09-15 00:15:00</td>\n",
       "      <td>1020</td>\n",
       "      <td>0.10</td>\n",
       "      <td>17031281900</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.879255</td>\n",
       "      <td>-87.642649</td>\n",
       "      <td>41.898332</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000d50f70b61120f8342e039410a6ce077db84d2</td>\n",
       "      <td>839c8a07639d9477fe43f3e2b7be014a3deead1930460c...</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>2019-08-24 23:45:00</td>\n",
       "      <td>478</td>\n",
       "      <td>1.35</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>17031071500</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.898332</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>41.914616</td>\n",
       "      <td>-87.631717</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000f9ef866c0f120314ceb0d9290e71e7c23fd6a</td>\n",
       "      <td>e93472aad9e00c0a523e1a861ae897303548d713ba63d3...</td>\n",
       "      <td>2019-08-01 05:30:00</td>\n",
       "      <td>2019-08-01 06:00:00</td>\n",
       "      <td>1657</td>\n",
       "      <td>2.64</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>17031320100</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Flash Cab</td>\n",
       "      <td>41.898332</td>\n",
       "      <td>-87.620763</td>\n",
       "      <td>41.884987</td>\n",
       "      <td>-87.620993</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010ea18098781e411e7ea4f92e400e7058de94c</td>\n",
       "      <td>93930278eb7df9672cdc277c92e59dae0f82116868ce1c...</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>2018-06-15 20:45:00</td>\n",
       "      <td>420</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17031071500</td>\n",
       "      <td>17031080202</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.914616</td>\n",
       "      <td>-87.631717</td>\n",
       "      <td>41.905858</td>\n",
       "      <td>-87.630865</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>166</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    trip_id  \\\n",
       "0  000040bde022bec832268c32707fd802c0a3d956   \n",
       "1  000b5c4c37a2c154b6f703803068625c67f81017   \n",
       "2  000d50f70b61120f8342e039410a6ce077db84d2   \n",
       "3  000f9ef866c0f120314ceb0d9290e71e7c23fd6a   \n",
       "4  0010ea18098781e411e7ea4f92e400e7058de94c   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  adb0da2862924ceb2be34ffd2eab0a097a8e6594c39415...  2018-06-14 10:00:00   \n",
       "1  0423b61dd9d4f4da3f4490e3dfed085a321aa78d527286...  2019-09-15 00:00:00   \n",
       "2  839c8a07639d9477fe43f3e2b7be014a3deead1930460c...  2019-08-24 23:45:00   \n",
       "3  e93472aad9e00c0a523e1a861ae897303548d713ba63d3...  2019-08-01 05:30:00   \n",
       "4  93930278eb7df9672cdc277c92e59dae0f82116868ce1c...  2018-06-15 20:45:00   \n",
       "\n",
       "   trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0 2018-06-14 10:15:00           600        0.80          17031839100   \n",
       "1 2019-09-15 00:15:00          1020        0.10          17031281900   \n",
       "2 2019-08-24 23:45:00           478        1.35          17031081300   \n",
       "3 2019-08-01 06:00:00          1657        2.64          17031081300   \n",
       "4 2018-06-15 20:45:00           420        0.00          17031071500   \n",
       "\n",
       "   dropoff_census_tract  pickup_community_area  dropoff_community_area  ...  \\\n",
       "0           17031320100                     32                      32  ...   \n",
       "1           17031081300                     28                       8  ...   \n",
       "2           17031071500                      8                       7  ...   \n",
       "3           17031320100                      8                      32  ...   \n",
       "4           17031080202                      7                       8  ...   \n",
       "\n",
       "                     company  pickup_centroid_latitude  \\\n",
       "0  Star North Management LLC                 41.880994   \n",
       "1  Taxi Affiliation Services                 41.879255   \n",
       "2  Chicago Carriage Cab Corp                 41.898332   \n",
       "3                  Flash Cab                 41.898332   \n",
       "4  Taxi Affiliation Services                 41.914616   \n",
       "\n",
       "   pickup_centroid_longitude  dropoff_centroid_latitude  \\\n",
       "0                 -87.632746                  41.884987   \n",
       "1                 -87.642649                  41.898332   \n",
       "2                 -87.620763                  41.914616   \n",
       "3                 -87.620763                  41.884987   \n",
       "4                 -87.631717                  41.905858   \n",
       "\n",
       "   dropoff_centroid_longitude  year month  day  dayofyear  hour  \n",
       "0                  -87.620993  2018     6   14        165    10  \n",
       "1                  -87.620763  2019     9   15        258     0  \n",
       "2                  -87.631717  2019     8   24        236    23  \n",
       "3                  -87.620993  2019     8    1        213     5  \n",
       "4                  -87.630865  2018     6   15        166    20  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check and show the columns available\n",
    "filtered_taxi_trips.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68849 trips happened on 14 June of 2018\n"
     ]
    }
   ],
   "source": [
    "single_day_trip_count=filtered_taxi_trips.filter((filtered_taxi_trips['dayofyear']==165) & (filtered_taxi_trips['year']==2018)).count()\n",
    "print(\" %s trips happened on 14 June of 2018\" % single_day_trip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional!] Casting to Timestamp\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "taxi_trips = taxi_trips.select(*[taxi_trips.columns]).withColumn(\"record_date\",taxi_trips[\"trip_end_timestamp\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional!] Convert column to unix timestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "taxi_trips = taxi_trips.withColumn(\n",
    "    \"start_time\", unix_timestamp(\"trip_start_timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
    "        ).withColumn(\n",
    "    \"end_time\", unix_timestamp(\"trip_end_timestamp\", \"yyyy-MM-dd HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b. Weather Data ETL\n",
    "\n",
    "Next, we split weather data into separate tables that will contain the most useful information and additional information about weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d868b0731ae9459cb72f46ad8ff055f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_time_data(df, date_column, unique_column):\n",
    "    \"\"\"\n",
    "    Extract year, dayofyear and month from timestamp field\n",
    "    \n",
    "    Returns: new dataset that can be joined by @unique_column\n",
    "    \"\"\"\n",
    "    col = date_column\n",
    "    days_df = df.select(year(col).alias('year'),  dayofyear(col).alias('dayofyear'), weekofyear(col).alias('month'), unique_column)\n",
    "    return days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can join extrated time data with weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd665813e824b4e94de677a17dcabff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "days = extract_time_data(weather_df, date_column=\"DATE\", unique_column=\"DATE\")\n",
    "weather_df = weather_df.join(days, on=\"DATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a set of columns are defined that will represent separate tables.\n",
    "<br>Tables are created next, as separate dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39b5338fc6e44888fc5247fedbb4990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "join_columns = [\"year\", \"dayofyear\", \"month\"]\n",
    "weather_temp_columns = [\"DATE\", \"AWND\", \"PRCP\", \"SNOW\", \"SNWD\", \"TMAX\", \"TMIN\", \"TAVG\"] + join_columns\n",
    "weather_conditions = [\"DATE\", \"WT01\", \"WT02\", \"WT03\", \"WT04\", \"WT05\", \"WT09\"] + join_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final **temperature** table looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25988a2079f4df784857b11ca412cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperature_df = weather_df.select(*[weather_temp_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>AWND</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TAVG</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-17.1</td>\n",
       "      <td>-22.7</td>\n",
       "      <td>-18.9</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>-22.7</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-8.2</td>\n",
       "      <td>-14.3</td>\n",
       "      <td>-11.4</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-17.7</td>\n",
       "      <td>-13.9</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-11.6</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  AWND  PRCP  SNOW  SNWD  TMAX  TMIN  TAVG  year  dayofyear  month\n",
       "0 2018-01-01   4.8   0.0   0.0  30.0 -17.1 -22.7 -18.9  2018          1      1\n",
       "1 2018-01-02   5.0   0.0   0.0  30.0 -13.2 -22.7 -19.1  2018          2      1\n",
       "2 2018-01-03   5.5   0.0   3.0  30.0  -8.2 -14.3 -11.4  2018          3      1\n",
       "3 2018-01-04   5.5   0.0   0.0  30.0 -11.0 -17.7 -13.9  2018          4      1\n",
       "4 2018-01-05   4.8   0.0   0.0  30.0 -11.6 -18.2 -15.2  2018          5      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather conditions data will contain many null values, but this is expected, \n",
    "<br>as this would indicate that weather was normal that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>WT01</th>\n",
       "      <th>WT02</th>\n",
       "      <th>WT03</th>\n",
       "      <th>WT04</th>\n",
       "      <th>WT05</th>\n",
       "      <th>WT09</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  WT01  WT02  WT03  WT04  WT05  WT09  year  dayofyear  month\n",
       "0 2018-01-01   NaN  None  None  None  None  None  2018          1      1\n",
       "1 2018-01-02   NaN  None  None  None  None  None  2018          2      1\n",
       "2 2018-01-03   1.0  None  None  None  None  None  2018          3      1\n",
       "3 2018-01-04   NaN  None  None  None  None  None  2018          4      1\n",
       "4 2018-01-05   NaN  None  None  None  None  None  2018          5      1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weather_df.select(*[weather_conditions]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e9a3b49ccc474cbe2a329b44a57db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Renaming several columns to have better representation of data\n",
    "conditions_df = weather_df.selectExpr(\"DATE\", \"year\", \"dayofyear\", \"month\", \"WT01 as FOG\", \"WT02 as HEAVY_FOG\", \"WT03 as THNDR\", \"WT04 as ICE\", \"WT05 as HAIL\", \"WT09 as HVSNOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- dayofyear: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- FOG: double (nullable = true)\n",
      " |-- HEAVY_FOG: double (nullable = true)\n",
      " |-- THNDR: double (nullable = true)\n",
      " |-- ICE: double (nullable = true)\n",
      " |-- HAIL: double (nullable = true)\n",
      " |-- HVSNOW: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>month</th>\n",
       "      <th>FOG</th>\n",
       "      <th>HEAVY_FOG</th>\n",
       "      <th>THNDR</th>\n",
       "      <th>ICE</th>\n",
       "      <th>HAIL</th>\n",
       "      <th>HVSNOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  year  dayofyear  month  FOG HEAVY_FOG THNDR   ICE  HAIL HVSNOW\n",
       "0 2018-01-01  2018          1      1  NaN      None  None  None  None   None\n",
       "1 2018-01-02  2018          2      1  NaN      None  None  None  None   None\n",
       "2 2018-01-03  2018          3      1  1.0      None  None  None  None   None\n",
       "3 2018-01-04  2018          4      1  NaN      None  None  None  None   None\n",
       "4 2018-01-05  2018          5      1  NaN      None  None  None  None   None"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditions_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible improvements**\n",
    "\n",
    "Data loaded was implicitly casted to appropriate type, that may allow us to extract day, weekday, hour, etc.\n",
    "<br>But weather data we use has coarse granularity, so it can be left for future improvements, or for datset that is updated evenry 15 minutes.\n",
    "<br>For our goals, the given dataset is more than enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Main Fact Table\n",
    "\n",
    "To create Fact table, we join temperature dataframe and overall taxi dataframe.\n",
    "<br>We also skip missing data, thus we only left with rows that completely filled with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SQL queries in this section, as this will simplify working with large amount of colums at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdf3f59dae145ccbc53d7f0399ee284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_taxi_trips.createOrReplaceTempView(\"trips\")\n",
    "temperature_df.createOrReplaceTempView(\"temps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we filter out missing location data.\n",
    "<br> **At least 30%** of Census Tract data were hidden from the dataset due to privacy reasons, however filtering missing data will reduce amount of data for analysis drastically.\n",
    "<br> We are interested in performing location suggestion with precision to block or individual street.\n",
    "<br> For now we will not completely filter out `census` columns.\n",
    "<br> Additional filters can be added on demand with: \n",
    "```sql\n",
    "WHERE pickup_census_tract IS NOT NULL \n",
    "AND dropoff_census_tract IS NOT NULL \n",
    "AND dropoff_community_area IS NOT NULL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1df052f5fd34210a7ecc303e9eb2787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "taxi_trips_cleaned = filtered_taxi_trips.filter((filtered_taxi_trips['pickup_centroid_latitude'].isNotNull()) & \\\n",
    "                            (filtered_taxi_trips['pickup_centroid_longitude'].isNotNull()) & \\\n",
    "                            (filtered_taxi_trips['dropoff_centroid_latitude'].isNotNull()) & \\\n",
    "                            (filtered_taxi_trips['dropoff_centroid_longitude'].isNotNull())                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same filtering call using SQL. As I noticed, more efficient on a big cluster, \n",
    "# na dmay fail on a small cluster due to out of memory of DAG taks exceptions \n",
    "\n",
    "taxi_trips_cleaned = spark.sql(\"\"\"SELECT * FROM trips \n",
    "                    WHERE pickup_centroid_latitude IS NOT NULL\n",
    "                    AND pickup_centroid_longitude IS NOT NULL\n",
    "                    AND dropoff_centroid_latitude IS NOT NULL\n",
    "                    AND dropoff_centroid_longitude IS NOT NULL\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803d2736f6454c139b9bf62f5f879697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+--------------------+------------------------+-------------------------+-------------------------+--------------------------+----+-----+---+---------+----+\n",
      "|             trip_id|             taxi_id|trip_start_timestamp| trip_end_timestamp|trip_seconds|trip_miles|pickup_census_tract|dropoff_census_tract|pickup_community_area|dropoff_community_area| fare|tips|tolls|extras|trip_total|payment_type|             company|pickup_centroid_latitude|pickup_centroid_longitude|dropoff_centroid_latitude|dropoff_centroid_longitude|year|month|day|dayofyear|hour|\n",
      "+--------------------+--------------------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+--------------------+------------------------+-------------------------+-------------------------+--------------------------+----+-----+---+---------+----+\n",
      "|00002ae49c6b3f89b...|f8885a88420906f7a...| 2019-12-16 13:30:00|2019-12-16 13:45:00|         435|       1.0|        17031081201|         17031081403|                    8|                     8| 6.25| 0.0|  0.0|   0.0|      6.25|        Cash|Chicago Independents|            41.899155613|            -87.626210532|             41.890922026|             -87.618868355|2019|   12| 16|      350|  13|\n",
      "|000040bde022bec83...|adb0da2862924ceb2...| 2018-06-14 10:00:00|2018-06-14 10:15:00|         600|       0.8|        17031839100|         17031320100|                   32|                    32| 7.25| 3.0|  0.0|   0.0|     10.25| Credit Card|Star North Manage...|            41.880994471|            -87.632746489|             41.884987192|             -87.620992913|2018|    6| 14|      165|  10|\n",
      "|00017985db01c327c...|8a42ae5cf4c0d46ab...| 2019-05-16 09:30:00|2019-05-16 09:45:00|         600|       0.0|        17031280100|         17031320400|                   28|                    32|  7.0| 0.0|  0.0|   0.0|       7.0|        Cash|Taxi Affiliation ...|            41.885300022|            -87.642808466|             41.877406123|             -87.621971652|2019|    5| 16|      136|   9|\n",
      "|00018d6c4e3b4150a...|2b4bd3ac2b2acea1f...| 2020-01-21 08:15:00|2020-01-21 08:30:00|        1080|       1.8|        17031081300|         17031281900|                    8|                    28|11.75| 0.0|  0.0|   0.0|     11.75|        Cash|    Medallion Leasin|            41.898331794|            -87.620762865|             41.879255084|             -87.642648998|2020|    1| 21|       21|   8|\n",
      "|0001a958e47497c11...|73671d8d04e67d4ea...| 2018-10-17 13:00:00|2018-10-17 13:00:00|          60|       0.0|               null|                null|                    6|                     6| 3.25| 2.0|  0.0|  17.5|     22.75| Credit Card|Blue Ribbon Taxi ...|            41.944226601|            -87.655998182|             41.944226601|             -87.655998182|2018|   10| 17|      290|  13|\n",
      "+--------------------+--------------------+--------------------+-------------------+------------+----------+-------------------+--------------------+---------------------+----------------------+-----+----+-----+------+----------+------------+--------------------+------------------------+-------------------------+-------------------------+--------------------------+----+-----+---+---------+----+"
     ]
    }
   ],
   "source": [
    "taxi_trips_cleaned.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note!** Run the checks if you really need to. This will trigger reshuffling of the data and count missing vlues over all rows.\n",
    "<br> It may take a very long time to finish if dataset is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows before processing further\n",
    "check_total_rows(taxi_trips_cleaned)\n",
    "\n",
    "# Here we can see if dataset was cleaned from missing values\n",
    "check_missing_data(taxi_trips_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see now if all missing values were to be cleaneed, the dataset is reduced to 60% of the originally available dataset.\n",
    "\n",
    "Cleaning only missing coordinate data will keep 85-90% of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_id',\n",
       " 'taxi_id',\n",
       " 'trip_start_timestamp',\n",
       " 'trip_end_timestamp',\n",
       " 'trip_seconds',\n",
       " 'trip_miles',\n",
       " 'pickup_census_tract',\n",
       " 'dropoff_census_tract',\n",
       " 'pickup_community_area',\n",
       " 'dropoff_community_area',\n",
       " 'fare',\n",
       " 'tips',\n",
       " 'tolls',\n",
       " 'extras',\n",
       " 'trip_total',\n",
       " 'payment_type',\n",
       " 'company',\n",
       " 'pickup_centroid_latitude',\n",
       " 'pickup_centroid_longitude',\n",
       " 'dropoff_centroid_latitude',\n",
       " 'dropoff_centroid_longitude',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'dayofyear',\n",
       " 'hour']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_trips_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE',\n",
       " 'AWND',\n",
       " 'PRCP',\n",
       " 'SNOW',\n",
       " 'SNWD',\n",
       " 'TMAX',\n",
       " 'TMIN',\n",
       " 'TAVG',\n",
       " 'year',\n",
       " 'dayofyear']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table = taxi_trips_cleaned.join(temperature_df, on = [\"year\",\"dayofyear\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>...</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>DATE</th>\n",
       "      <th>AWND</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>SNWD</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>TAVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>112</td>\n",
       "      <td>0c56851ee59396edbf08703fd101cad2661b61b3</td>\n",
       "      <td>5e00738ed97ac63a803381fb963c3b05e2b2f332cd707f...</td>\n",
       "      <td>2018-04-22 16:45:00</td>\n",
       "      <td>2018-04-22 16:45:00</td>\n",
       "      <td>360</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17031081300</td>\n",
       "      <td>17031081600</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>112</td>\n",
       "      <td>168038b6eb015565e2769d9e79f817434318861a</td>\n",
       "      <td>341d6e8bdcbbd41081743ae65c0b6cc962bb42c225971b...</td>\n",
       "      <td>2018-04-22 12:00:00</td>\n",
       "      <td>2018-04-22 12:15:00</td>\n",
       "      <td>300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17031081500</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>112</td>\n",
       "      <td>20bf0fd9f7efc533b3e54dd9b1f0703b57969b55</td>\n",
       "      <td>f7bec4705b1d00364fdc119b2662a2b68a51e9d88c5f95...</td>\n",
       "      <td>2018-04-22 10:45:00</td>\n",
       "      <td>2018-04-22 11:00:00</td>\n",
       "      <td>540</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17031080300</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>112</td>\n",
       "      <td>235f71e1f7a0310c2bccbc6dba2330f57e99d2fe</td>\n",
       "      <td>97d540f40171ccb4795908136c5d5139aa925e1aedcc41...</td>\n",
       "      <td>2018-04-22 12:00:00</td>\n",
       "      <td>2018-04-22 12:00:00</td>\n",
       "      <td>720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031330100</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>112</td>\n",
       "      <td>2b88cec89c290f6beb44c0467a8666d5424a91bf</td>\n",
       "      <td>8b07f9156e568a37d362463c84dbd1118b4eeb753bae50...</td>\n",
       "      <td>2018-04-22 14:00:00</td>\n",
       "      <td>2018-04-22 14:45:00</td>\n",
       "      <td>2700</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17031081403</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  dayofyear                                   trip_id  \\\n",
       "0  2018        112  0c56851ee59396edbf08703fd101cad2661b61b3   \n",
       "1  2018        112  168038b6eb015565e2769d9e79f817434318861a   \n",
       "2  2018        112  20bf0fd9f7efc533b3e54dd9b1f0703b57969b55   \n",
       "3  2018        112  235f71e1f7a0310c2bccbc6dba2330f57e99d2fe   \n",
       "4  2018        112  2b88cec89c290f6beb44c0467a8666d5424a91bf   \n",
       "\n",
       "                                             taxi_id trip_start_timestamp  \\\n",
       "0  5e00738ed97ac63a803381fb963c3b05e2b2f332cd707f...  2018-04-22 16:45:00   \n",
       "1  341d6e8bdcbbd41081743ae65c0b6cc962bb42c225971b...  2018-04-22 12:00:00   \n",
       "2  f7bec4705b1d00364fdc119b2662a2b68a51e9d88c5f95...  2018-04-22 10:45:00   \n",
       "3  97d540f40171ccb4795908136c5d5139aa925e1aedcc41...  2018-04-22 12:00:00   \n",
       "4  8b07f9156e568a37d362463c84dbd1118b4eeb753bae50...  2018-04-22 14:00:00   \n",
       "\n",
       "   trip_end_timestamp  trip_seconds  trip_miles  pickup_census_tract  \\\n",
       "0 2018-04-22 16:45:00           360         0.0          17031081300   \n",
       "1 2018-04-22 12:15:00           300         0.0          17031081500   \n",
       "2 2018-04-22 11:00:00           540         2.0          17031080300   \n",
       "3 2018-04-22 12:00:00           720         0.0          17031839100   \n",
       "4 2018-04-22 14:45:00          2700        17.5          17031081403   \n",
       "\n",
       "   dropoff_census_tract  ...  day  hour       DATE  AWND  PRCP  SNOW  SNWD  \\\n",
       "0           17031081600  ...   22    16 2018-04-22   4.7   0.0   0.0   0.0   \n",
       "1           17031839100  ...   22    12 2018-04-22   4.7   0.0   0.0   0.0   \n",
       "2           17031839100  ...   22    10 2018-04-22   4.7   0.0   0.0   0.0   \n",
       "3           17031330100  ...   22    12 2018-04-22   4.7   0.0   0.0   0.0   \n",
       "4           17031980000  ...   22    14 2018-04-22   4.7   0.0   0.0   0.0   \n",
       "\n",
       "   TMAX TMIN  TAVG  \n",
       "0  16.7  5.6  10.1  \n",
       "1  16.7  5.6  10.1  \n",
       "2  16.7  5.6  10.1  \n",
       "3  16.7  5.6  10.1  \n",
       "4  16.7  5.6  10.1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Fact Table can be created that summarizes all data of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of the join performed above with Spark SQL\n",
    "taxi_trips_cleaned.createOrReplaceTempView(\"new_trips\")\n",
    "temperature_df.createOrReplaceTempView(\"temps\")\n",
    "\n",
    "fact_table = spark.sql(\"\"\"SELECT * FROM new_trips nt \n",
    "                 INNER JOIN temps ON nt.year = temps.year AND nt.dayofyear = temps.dayofyear\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe is available\n"
     ]
    }
   ],
   "source": [
    "check_dataframe_correctness(fact_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4c. Saving data in S3 Data Lake\n",
    "\n",
    "Data lake will be located in S3 bucket and folder will constitute names for separate data models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 output data location\n",
    "\n",
    "data_lake_name = \"/storage/chicago-extended-taxi-data-lake-capstone/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended Chicago taxi data, can be used for machine Learning w/o filtering by hidden `Census tract` fiedlds (pickup location, dropoff location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we going to partition data by day and year, as there are \n",
    "taxi_trips.write.mode(\"append\").partitionBy([\"year\",\"dayofyear\"]).parquet(data_lake_name + \"/data/taxi_trips/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collected data for each taxi cab, with detailed information for each ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabs_df.write.mode(\"append\").partitionBy([\"taxi_id\"]).parquet(data_lake_name + \"/data/cabs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of popular pickup locations in each Community Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_areas_df.write.mode(\"append\").partitionBy(\"community_area\").parquet(data_lake_name + \"/data/locations/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature and conditions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_df.write.mode(\"overwrite\").partitionBy([\"year\",\"month\"]).parquet(data_lake_name + \"/data/temperature/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_df.write.mode(\"overwrite\").partitionBy([\"year\",\"month\"]).parquet(data_lake_name + \"/data/weather_conditions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.write.mode(\"overwrite\").partitionBy([\"year\",\"month\"]).parquet(data_lake_name + \"/data/full_weather_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Data Lake Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table.write.mode('append').partitionBy([\"year\",\"dayofyear\"]).parquet(data_lake_name + \"/data/extended_chicago_taxi_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each section the number of lines loaded is checked, and also before and after processing and joining the dataframes.\n",
    "\n",
    "Additionally, datasets are checked for missing values and datasets that cannot contain missing values are processed accordingly.\n",
    "\n",
    "Data integrity is checked during first step, to show overall summary of each column.\n",
    "\n",
    "Example call are described below. This can be run for the final dataaset of the each dataset of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dataframe_correctness(fact_table)\n",
    "\n",
    "check_missing_data(taxi_trips_cleaned)\n",
    "\n",
    "check_total_rows(fact_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "Next line will ensure that tables are rendered correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Fact Table\n",
    "Cleaned Taxi Trip data joined with Weather Data (temperature and amount of Precipitation, Snow, etc.)\n",
    "\n",
    "Colums of **extended_chicago_taxi_rides** table:\n",
    "\n",
    " |column| type| description\n",
    " |:-----|:----|:-----------\n",
    " |trip_id| string|  A unique identifier for the trip.\n",
    " |taxi_id| string| A unique identifier for the taxi.\n",
    " |trip_start_timestamp| timestamp| When the trip started, rounded to the nearest 15 minutes.\n",
    " |trip_end_timestamp| timestamp| When the trip ended, rounded to the nearest 15 minutes.\n",
    " |day_of_week| integer| Day of week when trip started\n",
    " |month|int| Month when trip started\n",
    " |hour|int| Hour when trip started. As data rounded to 15 minutes, we don't need more precision\n",
    " |trip_seconds| integer| Time of the trip in seconds.\n",
    " |trip_miles| double|  Distance of the trip in miles.\n",
    " |pickup_census_tract| long| The Census Tract where the trip began.\n",
    " |dropoff_census_tract| long | The Census Tract where the trip ended.\n",
    " |pickup_community_area| integer | The Community Area where the trip began\n",
    " |dropoff_community_area| integer | The Community Area where the trip ended. \n",
    " |fare| double | The fare for the trip.\n",
    " |tip| integer | The tip for the trip. Cash tips generally will not be recorded.\n",
    " |additional_charges| double | The tolls for the trip.\n",
    " |extra| double |Extra charges for the trip.\n",
    " |trip_total| double | Total cost of the trip, the total of the previous columns.\n",
    " |payment_type| string| Type of payment for the trip.\n",
    " |taxi_company| string | The taxi company.\n",
    " |pickup_centroid_latitude| double | The latitude of the center of the pickup census tract.\n",
    " |pickup_centroid_longitude| double | The longitude of the center of the pickup census tract.\n",
    " |dropoff_centroid_latitude| double | The latitude of the center of the dropoff census tract.\n",
    " |dropoff_centroid_longitude| double | The longitude of the center of the dropoff census tract.\n",
    " |AWND | double    | Average wind speed\n",
    " |PRCP | double    | Precipitation\n",
    " |SNOW | double    | Snowfall\n",
    " |SNWD | double    | Snow depth\n",
    " |TMAX | double    | Maximum temperature\n",
    " |TMIN | double    | Minimum temperature\n",
    " |TAVG | double    | Average temperature\n",
    "\n",
    "### 1st Dimension Table\n",
    "Table will contain information about Taxi Cabs, trips they performed and general infofrmation about each trip.\n",
    "\n",
    "Colums of **cabs** table:\n",
    "\n",
    " |column| type| description\n",
    " |:-----|:----|:-----------\n",
    " |taxi_id| string| A unique identifier for the taxi.\n",
    " |trip_id| string|  A unique identifier for the trip.\n",
    " |trip_start_timestamp| timestamp| When the trip started, rounded to the nearest 15 minutes.\n",
    " |trip_end_timestamp| timestamp| When the trip ended, rounded to the nearest 15 minutes.\n",
    " |trip_seconds| integer| Time of the trip in seconds.\n",
    " |trip_miles| double|  Distance of the trip in miles.\n",
    " |pickup_community_area| integer | The Community Area where the trip began\n",
    " |dropoff_community_area| integer | The Community Area where the trip ended. \n",
    " |fare| double | The fare for the trip.\n",
    " |tip| integer | The tip for the trip. Cash tips generally will not be recorded.\n",
    " |additional_charges| double | The tolls for the trip.\n",
    " |extra| double |Extra charges for the trip.\n",
    " |trip_total| double | Total cost of the trip, the total of the previous columns.\n",
    " |payment_type| string| Type of payment for the trip.\n",
    " |taxi_company| string | The taxi company.\n",
    " \n",
    " \n",
    "### 2nd Dimension Table\n",
    "Table that collects popular locations in each community area in Chicago city.\n",
    "\n",
    "May be used by Taxi Drivers to choose better location for pickups during time of the day and weather conditions at the given moment.\n",
    "\n",
    "Colums of **community_areas** table:\n",
    "\n",
    " |column| type| description\n",
    " |:-----|:----|:-----------\n",
    " | community_area| integer | The Community Area id\n",
    " | centroid_latitute | double | Latitude of the location inside the Community Area (rounded to 20 meters)\n",
    " | centroid_longitude | double | Longitude of the location inside the Community Area (rounded to 20 meters)\n",
    "\n",
    "\n",
    "### 3d Dimension Table for Machine Learning Team\n",
    "Table with taxi data with missing values for `Census Tract` and `Location`\n",
    "\n",
    "Colums of **taxi_data** table:\n",
    "\n",
    " |column| type| description\n",
    " |:-----|:----|:-----------\n",
    " |trip_id| string|  A unique identifier for the trip.\n",
    " |taxi_id| string| A unique identifier for the taxi.\n",
    " |trip_start_timestamp| timestamp| When the trip started, rounded to the nearest 15 minutes.\n",
    " |trip_end_timestamp| timestamp| When the trip ended, rounded to the nearest 15 minutes.\n",
    " |trip_seconds| integer| Time of the trip in seconds.\n",
    " |trip_miles| double|  Distance of the trip in miles.\n",
    " |pickup_census_tract| long| The Census Tract where the trip began.\n",
    " |dropoff_census_tract| long | The Census Tract where the trip ended.\n",
    " |pickup_community_area| integer | The Community Area where the trip began\n",
    " |dropoff_community_area| integer | The Community Area where the trip ended. \n",
    " |fare| double | The fare for the trip.\n",
    " |tip| integer | The tip for the trip. Cash tips generally will not be recorded.\n",
    " |additional_charges| double | The tolls for the trip.\n",
    " |extra| double |Extra charges for the trip.\n",
    " |trip_total| double | Total cost of the trip, the total of the previous columns.\n",
    " |payment_type| string| Type of payment for the trip.\n",
    " |taxi_company| string | The taxi company.\n",
    " |pickup_centroid_latitude| double | The latitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    " |pickup_centroid_longitude| double | The longitude of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    " |pickup_centroid_location| string | The location of the center of the pickup census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    " |dropoff_centroid_latitude| double | The latitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    " |dropoff_centroid_longitude| double | The longitude of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    " |dropoff_centroid_location| string | The location of the center of the dropoff census tract or the community area if the census tract has been hidden for privacy. This column often will be blank for locations outside Chicago.\n",
    "\n",
    "### Secondary Weather Fact Table\n",
    "Table about weather data and data conditions.\n",
    "\n",
    "Joined table of **1st Weather Dimension Table** and **2nd Weather Dimension Table**, both described below.\n",
    "\n",
    "Table schema left out for readability and to avoid duplication.\n",
    "\n",
    "Colums of **weather** table - joined columns of **temperature** and **conditions** tables by DATE coulmn.\n",
    "\n",
    "This table is not of particular interest for our needs, as mostly we are going to work with tables defined below\n",
    "\n",
    "### 1st Weather Dimension Table\n",
    "Table that contains temperature and precipitation data\n",
    "\n",
    "Colums of **temperatures** table:\n",
    "\n",
    "|column | type  | description |\n",
    "|:-------|:-------|:-|\n",
    "|DATE | timestamp | Date of measurements\n",
    "|AWND | double    | Average wind speed\n",
    "|PRCP | double    | Precipitation\n",
    "|SNOW | double    | Snowfall\n",
    "|SNWD | double    | Snow depth\n",
    "|TMAX | double    | Maximum temperature\n",
    "|TMIN | double    | Minimum temperature\n",
    "|TAVG | double    | Average temperature\n",
    "\n",
    "\n",
    "### 2nd Weather Dimension Table\n",
    "Table with weather conditions data for each day\n",
    "\n",
    "Colums of **conditions** table:\n",
    "\n",
    "column | type  | description |\n",
    ":------|:-------|:-|\n",
    "DATE| timestamp| Date of measurements\n",
    "FOG| int | Fog during the  day\n",
    "HEAVY_FOG| int | Heavy or freezing fog during the day\n",
    "THNDR| int | Thunder during the day\n",
    "ICE| int | Ice pellets, sleet, snow pellets, or small hail\n",
    "HAIL| int| Hail during the day\n",
    "HVSNOW|int | Blowing or drifting snow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technologies\n",
    "* Python: Best choice for exploratory data analysis and integration between various technologies.\n",
    "* Spark (EMR cluster): for running ETL Pipelines on large datasets (20 Gb).\n",
    "* S3: fast and highly available storage solution. Can be easily integrated with Redshift or NoSQL database, such as Cassanda (as our data models can by tweaked for exact queries that need to be performed on data).\n",
    "\n",
    "This example was run in EMR cluster, as well as in local cluster.\n",
    "\n",
    "EMR clusters, provider better scalability and require much less effort to setup infrastructure correctly.\n",
    "\n",
    "#### Update Cycles\n",
    "\n",
    "Taxi Trips: An official dataset is updated monthly, so it would make sense to update data on a monthly basis and append to existing tables.\n",
    "\n",
    "However if similar datset is available 24/7, it would make sense to update it at least daily (or weekly), to provide as much useful insights for passengers, cab drivers, taxi companies as well as for Chicago Transport official institutions.\n",
    "\n",
    "Weather: can be updated daily, however monthly updates is enough (and this will match updates to main taxi dataset), as dataset is small but *wide* (with many columns that provide additional information). \n",
    "\n",
    "\n",
    "#### Scenarios\n",
    "**The data was increased by 100x**\n",
    "<br>To be able to query data faster, more nodes to Spark cluster should be added to handle the load.\n",
    "<br>Another option is to set up a Streaming solutio to ingest and buffer the incoming trid data.\n",
    "<br>This can be also achieved with introducting additional Airflow Pipelines that would process only next incoming batch of data (e.g. on hourly or daily schedule), and store it in S3 that can be further queried with Spark cluster (EMR) with less nodes.\n",
    "\n",
    "**The data populates a dashboard that must be updated on a daily basis by 7am every day**\n",
    "<br>Solution is similar to previous one. \n",
    "<br>A nightly job, for example, can be triggered in airflow that would run necessary pipeline and populate the dashboard.\n",
    "\n",
    "**The database needed to be accessed by 100+ people**\n",
    "<br>Using distributed database that can handle large number of concurrent connections is the way to go here.\n",
    "\n",
    "A very good use case here would be Redshift with several nodes, with at least 16 CPUs each.\n",
    "<br> Cassandra is might be even better, as data model defined in this project would allow to run queries on demand and database will scale automatically due to discributed nature of Cassandra (of course if cluster with at least 10 nodes is set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
